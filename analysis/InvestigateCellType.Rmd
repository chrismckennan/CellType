---
title: "Investigate cell type and how it relates to covariates"
author: "Chris McKennan"
date: 2016-02-05
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=T}
source("chunk-options.R")
source("../R/OptimizeLogLike.R")
source("../R/OptimizeLogLike_Ksigma.R")
```

The purpose of this file is to investigate how cell type is related to the covariates. That is, do we have any hope in predicting cell type given an individuals covariate information. In order to apply our supervised method, we need the variance in cell type among unrelated individuals to be small, given the covariate information.

## Load required functions

```{r}
library(minfi)
library('IlluminaHumanMethylation450kmanifest')
library('IlluminaHumanMethylation450kanno.ilmn12.hg19')
library('FlowSorted.Blood.450k')      ##Methylation data on 6 males, 10 cell types; RGset object
library('RefFreeEWAS')
library('nlme')
library('corpcor')
library('sva')
library('knitr')
library('printr')
```


## Get data into R

Get the data into R and initialize global variables.

```{r}
path.LPS.kids <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/HTkids_cellprop/HTkids_LPS_covar_ImpAct_10115.txt'
path.methylation <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/Hutterite_Methylation_data/Meth_covar_allsamp_impcells_10115.txt'
data.cov <- data.frame(read.table(path.methylation, sep="\t", quote="\"", dec=".", header=T, check.names=F))
data.LPS <- data.frame(read.table(path.LPS.kids, sep="\t", quote="\"", dec=".", header=T, check.names=F))
cell.names <- colnames(data.LPS)[(ncol(data.LPS) - 5):ncol(data.LPS)]
n.cells <- length(cell.names)
n.ind <- nrow(data.LPS)
ind.ids.cell <- data.LPS$Rowid    #IDs of individuals with cell type data
ind.ids.covar <- data.cov$Rowid   #IDs of individuals in complete covariate file
```

## How well do covariates predict cell type
Cell types are on regular 0-1 scale

```{r}
age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
model.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
H.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
Y.mat <- as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])
Y.mat.hat <- H.cov %*% Y.mat
beta.mat <- beta.op.cov %*% Y.mat
var.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))

i = 3
plot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))
lines(seq(0,100, 100), seq(0, 100, 100))

var.beta.hat <- diag(1/diag(solve(t(model.mat.cov) %*% model.mat.cov)))
t.scores <- sqrt(var.beta.hat) %*% beta.mat %*% diag(1/sqrt(var.vec))

```


##Multivariate Gaussian with Dispersed Multinomial Covariance Structure

The model I fit below assumes that for $p_i = \Omega^T x_i$,
\[
c_i \sim N\left(p_i, \Sigma_i\right), \quad \Sigma_i = \text{diag}\left(\sigma_1, \ldots, \sigma_K\right) \left(\text{diag}\left( p_i \right) - p_i p_i^T\right) \text{diag}\left(\sigma_1, \ldots, \sigma_K\right)
\]

First, I will get an estimate of $\sigma_k^2$ using the above simple univariate regressions. This will be used as a starting point in the optimization procedure. If $v_k$ is the variance in the OLS regression performed above, then $\sigma_k^2 = \frac{v_k}{p(1-p)}$, where $p$ is the cell type proportion. We need this to be small for our optimization/downstream method to perform well.
```{r Estimate sigma2}
var.OLS <- var.vec/1e4    #Michelle's proportions were given in %
ave.hat <- apply(Y.mat.hat, 2, mean)/100   #Average predicted proportion = average measured cell type proportion across individuals
sigma2.vec <- var.OLS/ave.hat/(1-ave.hat)
sigma2.0 <- median(sigma2.vec)   #Starting value of sigma^2 for optimization procedure. It corresponds to about 38 observations (a little small, although enough to make the ).
```

Since our naive estimated $\sigma^2$ is relatively large compared to the measured B cells and monocytes are relatively small (~1% and 0.5%, respectively), we do not have the power to accurately estimate their means. Thereore, we should consider combining them into the 'other' category. Eosinophil's are right on the border of what we have the power to estimate (note that these have been linked to the devlopment of asthma, so it may be crucial that we include them).

In this next part, I will get starting values for the mean. I will pool B cells and Monocytes into the 'Other' category.
```{r Get Initial Estiamats for the mean}
pool.cells <- c('Bcell.act', 'Mono.act')
C <- Y.mat[,! colnames(Y.mat) %in% pool.cells]
C[,which(colnames(C)=='Other.act')] = C[,which(colnames(C)=='Other.act')] + apply(Y.mat[,colnames(Y.mat) %in% pool.cells], 1, sum)
C <- C[,-which(colnames(C)=='Other.act')]/100   #C is cell type matrix to be used in optimization
Omega.0 <- beta.mat[,! colnames(Y.mat) %in% pool.cells]/100
Omega.0 <- Omega.0[,-which(colnames(Omega.0)=='Other.act')]

age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
X <- as.matrix(model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS))
```

Optimize the likelihood using a Newton line-search with $K$ dispersion parameters
```{r Optimize Likelihood}
n <- nrow(X)
d <- ncol(X)
K <- ncol(C)
grad.tol <- 1e-10   #Gradient tolerance
ML <- MaxLike.NewtonLS_Ksigma(X, C, Omega.0, rep(sigma2.0^0.5,K), grad.tol)
Omega <- ML$Omega    #A covariate x cell type matrix
FI <- ML$I     #Fisher information matrix at optimum
cov.theta <- solve(FI)
sigma <- ML$sigma    #sigma (i.e. #multinomial observations ~ 1/sigma^2)
mean.cell <- X %*% Omega
stand.Omega <- Omega/matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)
sd.Omega <- matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)
colnames(sd.Omega) <- colnames(Omega)
rownames(sd.Omega) <- rownames(Omega)

tmp.sigma <- matrix(sigma, nrow=1, ncol=K)
colnames(tmp.sigma) <- paste0('sigma_', as.character((1:K)))
tmp.pred.sigma <- rbind(Omega[1,])/(sqrt(rbind(Omega[1,]*(1-Omega[1,]))) * tmp.sigma)
knitr::kable(tmp.sigma, digits=4, caption="Estimates for sigma's", longtable=F)
knitr::kable(tmp.pred.sigma, digits=4, caption="Standardized Intercepts (i.e. how accurate is our prediction)", longtable=F)
knitr::kable(head(Omega.0), digits = 4, caption = "Omega using Univariate OLS", longtable=F)
knitr::kable(head(Omega), digits = 4, caption = "Omega using Multivariate Maximum Likelihood", longtable=F)
t.scores.table <- t.scores[,! colnames(Y.mat) %in% c('Other.act', pool.cells)]
colnames(t.scores.table) <- colnames(Omega)
rownames(t.scores.table) <- rownames(Omega)
knitr::kable(head(sd.Omega), digits = 4, caption = "SD of Estimated Effects using Fisher Information of ML Estimate", longtable=F)
knitr::kable(head(stand.Omega), digits = 4, caption = "Standardized Effects using Fisher Information of ML Estimate", longtable=F)
```

##Are our data overdispersed?
While the estimate $\hat{\sigma_k}^2_{\text{ML}}$ is the maximum likelihood estimate, I want to make sure the variance of the residuals is not is not larger than we would expect from our ML model (this should not be the case, as our estimated parameters make the data the most 'normal'). Let $\hat{\mu}_i = \hat{\Omega}_{\text{ML}}^T x_i$, $S_i^T S_i = \hat{\Sigma}_i = \hat{\sigma}^2_{\text{ML}}\left[\text{diag}\left( \hat{\mu}_i \right) - \hat{\mu}_i \hat{\mu}_i^T \right]$ and the residual for individual $i$ be $\hat{r}_i = c_i - \hat{\mu}_i$. Then $S_i^{-T} \hat{r}_i$ is approximately $N(0,1)$. Using standard multivariate regression theory, we can estimate the dispersion as
\[
\text{disp} = \left(\frac{\text{det}\left( \frac{1}{n-K}\hat{R}^T \hat{R}\right)}{E\det(W_K)}\right)^{1/K}
\]
where $W_K \sim \mathcal{W}_K \left( I_K, n-d\right)$. We (obviously) find that there is no dispersion with our estimated parameters.

```{r Estimate Overdispersion}
Resid <- C - mean.cell
Stand.Resid <- C - mean.cell
for (i in 1:n) {
  p.i <- mean.cell[i,]
  Sigma.i <- diag(sigma) %*% (diag(p.i) - cbind(p.i) %*% rbind(p.i)) %*% diag(sigma)
  R.i <- chol(Sigma.i)   #R.i'R.i = Sigma.i
  Stand.Resid[i,] = as.vector( solve(t(R.i), Resid[i,]) )   #Standardized residuals should be iid N(0,1)
}

##Look at distribution of det(Wishart) to get an idea how large Stand.Resid are##
#Note E|W| = n^{-p} * n * ... * (n - p + 1), where W ~ Wishart_p(I_p, n), since n^p|W| ~ \chi^2_n * ... \chi^2_{n-p+1}, where the \chi^2 are independent
precision.wish <- n
mean.detW <- exp( sum(log( (precision.wish:(precision.wish - K + 1)) )) ) * precision.wish^(-K)
disp <- (det(t(Stand.Resid) %*% Stand.Resid/precision.wish) / mean.detW)^(1/K)
#sigma2.final <- sigma2 * disp
```

Using the Fisher information at our estimated parameters as an estimate of their covariance, we come to the same conclusion about their significance using the univariate regression (the Fisher information includes the estimate for $\sigma_k^2$). I think this is a feasible method to estimate cell-type, provided we have enough individuals. The only problem is that for Eos.act, $\hat{\sigma_k}_{\text{ML}} = 0.14$, meaning our predictions will protrude through the simplex 93% of the time...

We might be able to use this as a rough estimate for the variance of our predictions. That way, we can determine how much individuals with unobserved cell types will help in our estimation.

##Probit-transform of the data
The only statistically significant predictors in the univariate regression are the intercept in all regressions (obviously) and the effect on age in 'other' cells. Maybe we can completely ignore the fact that these depend on covariates? They do not appear to depend on them...

cell types are probit-transformed:

```{r}
age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
model.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
H.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
Y.mat <- qnorm(as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])/100)
Y.mat.hat <- H.cov %*% Y.mat
beta.mat <- beta.op.cov %*% Y.mat
var.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))

i = 3
plot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))
lines(seq(0,100, 100), seq(0, 100, 100))
```

The conclusion is that regardless of scale (probit-transformed or regular scale), the current covariates (age, sex, asthma status) are a poor predictor of cell type. It also looks like we can get away with binning ages.

## Is logit(beta value) an appropriate scale to do regression against cell type (hopefully yes, so we can assume a normal response)

Load Michele's data

```{r}
#Load SWAN-normalized data
#Variables are Mset.swan.norm and rgset
load(file="/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/MyWork/SavedObjects/Rfile.RData")
meth.norm <- getMeth(Mset.swan.norm)
unmeth.norm <- getUnmeth(Mset.swan.norm)
beta.values <- meth.norm/(meth.norm+unmeth.norm+100)  #These are ordered by row in data.cov
```

Identify columns of the data that correspond to the data that I have in data.LPS (i.e. cell type data) and perform a regression

```{r}
col.use <- match(ind.ids.cell, ind.ids.covar)
beta.cell <- beta.values[,col.use]
M.cell <- log(beta.cell/(1-beta.cell))
model.mat <- model.matrix(~Tcell.act + Bcell.act + Eos.act + Neut.act + Mono.act, data=data.LPS)
beta.op <- solve(t(model.mat) %*% model.mat, t(model.mat))
H <- model.mat %*% beta.op
H.perp <- diag(1,ncol(H)) - H

coef.beta <- beta.op %*% t(beta.cell)
coef.M <- beta.op %*% t(M.cell)

SSreg.beta <- apply(beta.cell %*% H, 1, var)
SSreg.M <- apply(M.cell %*% H, 1, var)
SSY.beta <- apply(beta.cell, 1, var)
SSY.M <- apply(M.cell, 1, var)

R2.beta <- SSreg.beta/SSY.beta
R2.M <- SSreg.M/SSY.M

plot(R2.beta, R2.M, pch=".", xlim=range(R2.M, R2.beta), ylim=range(R2.M, R2.beta), main='Comparison of Variance Explained by Cell type', xlab='R^2 when Y = beta value', ylab='R^2 when Y = M value')
lines(seq(0,1,length=100), seq(0,1,length=100), col="red")
```

From this, it looks like we can use M-values when dealing with confounding due to cell type. It also appears cell type is NOT very dependent on these coefficients, but they may be dependent in other data sets.

## How well does Houseman's method do in predicting cell type?

I now want to look at how well Houseman's method perform's in predicting cell type in comparison to using training data. If it does well, there is no need to use training data. We can just use Houseman's trainin data.

How Tcell prediction differs between Houseman's 2012 method and a simple regression using Hutterite cell proportions (I added Houseman's CD4 and CD8 estimates to get his 'Tcell' proportion)
```{r}
House.cell <- data.LPS[(ncol(data.LPS)-12):(ncol(data.LPS)-7)]
Tcell.house <- House.cell[,1] + House.cell[,2]
Tcell.act <- data.LPS$Tcell.act
risk.house <- sum((Tcell.house*100 - Tcell.act)^2)/length(Tcell.act)

Tcell.hat <- H.cov %*% cbind(Tcell.act)
LOO.cv <- sqrt(mean( (Tcell.act - Tcell.hat)^2/(1 - diag(H.cov))^2 ))
print(paste0('Average L2 Loss for Houseman Tcells = ', as.character(signif(risk.house, digits=4))))
print(paste0('Average LOO CV L2 Loss for Regression Tcells = ', as.character(signif(LOO.cv, digits=4))))
```

Now with Bcells:
```{r}
Bcell.house <- House.cell$Bcell
Bcell.act <- data.LPS$Bcell.act
B.risk.house <- sum((Bcell.house*100 - Bcell.act)^2)/length(Bcell.act)

Bcell.hat <- H.cov %*% cbind(Bcell.act)
B.LOO.cv <- sqrt(mean( (Bcell.act - Bcell.hat)^2/(1 - diag(H.cov))^2 ))
print(paste0('Average L2 Loss for Houseman Bcells = ', as.character(signif(B.risk.house, digits=4))))
print(paste0('Average LOO CV L2 Loss for Regression Bcells = ', as.character(signif(B.LOO.cv, digits=4))))
```

## Conclusion

1. Houseman's training data does a poor job in predicting cell type in comparison to a simple linear regression with cell proportion training data.

2. At least in the Hutterite data set, cell proportion looks to be independent of asthma status, age (all individuals were 7-14 years old) and gender. They may be more dependent on covariates for different data sets, however.

3. We can estimate cell proportions using a Gaussian model with multinomial likelihood. With enough individuals we can get a good estimate of $\hat{\Omega}$. However, we estimate $\sigma \approx 0.18$, meaning our prediction may portrude through the simplex (see standardized effects table above).

4. We still need to understand how much additional information we can get from individuals with unmeasured cell types to estimate main effects of interest.


##How Much Additional Information do we get form Individuals with Unobserved Cell Types?
We suppose we have two groups of individuals, one with observed cell types and one without cell types. We model both these groups as
\[
Y_1 = B_{p \times d}X_1 + L_{p \times k}C_1^T + E_1, \quad Y_1 \in \mathbb{R}^{p \times n_1}
\]
\[
Y_2 = BX_2 + L\Omega_{K \times d}^T X_2 + L\Xi_{K \times n_2} + E_2, \quad Y_2 \in \mathbb{R}^{p \times n_2}.
\]
We assume that the columns of $E_i$ are indpendent. For now, $\Xi_{K \times n_2} = \left( \xi_1 \cdots \xi_{n_2} \right)$, where $\xi_i \sim N_K\left( 0,\Sigma_i \right)$, $p_i = \Omega^T_{K \times d}x_i$ and
\[
\Sigma_i = \text{diag}\left(\sigma_1, \ldots, \sigma_K\right) \left(\text{diag}\left( p_i \right) - p_i p_i^T\right) \text{diag}\left(\sigma_1, \ldots, \sigma_K\right).
\]
We first suppose that $L_{p \times K}$ with rows $\ell_g$, $g = 1, \ldots, p$ and $\Omega_{d \times K}$ are known. We are interested in determining how must additional information $Y_2$ gives us, which can be measured by how much the variance of $\hat{B}$ shrinks with the additional observations.

## Session information

```{r info}
sessionInfo()
```
