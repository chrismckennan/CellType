---
title: "Investigate cell type and how it relates to covariates"
author: "Chris McKennan"
date: 2016-02-05
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=T}
source("chunk-options.R")
source("../R/OptimizeLogLike.R")
source("../R/OptimizeLogLike_Ksigma.R")
```

The purpose of this file is to investigate how cell type is related to the covariates. That is, do we have any hope in predicting cell type given an individuals covariate information. In order to apply our supervised method, we need the variance in cell type among unrelated individuals to be small, given the covariate information.

## Load required functions

```{r}
library(minfi)
library('IlluminaHumanMethylation450kmanifest')
library('IlluminaHumanMethylation450kanno.ilmn12.hg19')
library('FlowSorted.Blood.450k')      ##Methylation data on 6 males, 10 cell types; RGset object
library('RefFreeEWAS')
library('nlme')
library('corpcor')
library('sva')
library('knitr')
library('printr')
```


## Get data into R

Get the data into R and initialize global variables.

```{r}
path.LPS.kids <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/HTkids_cellprop/HTkids_LPS_covar_ImpAct_10115.txt'
path.methylation <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/Hutterite_Methylation_data/Meth_covar_allsamp_impcells_10115.txt'
data.cov <- data.frame(read.table(path.methylation, sep="\t", quote="\"", dec=".", header=T, check.names=F))
data.LPS <- data.frame(read.table(path.LPS.kids, sep="\t", quote="\"", dec=".", header=T, check.names=F))
cell.names <- colnames(data.LPS)[(ncol(data.LPS) - 5):ncol(data.LPS)]
n.cells <- length(cell.names)
n.ind <- nrow(data.LPS)
ind.ids.cell <- data.LPS$Rowid    #IDs of individuals with cell type data
ind.ids.covar <- data.cov$Rowid   #IDs of individuals in complete covariate file
```

## How well do covariates predict cell type
Cell types are on regular 0-1 scale

```{r}
age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
model.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
H.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
Y.mat <- as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])
Y.mat.hat <- H.cov %*% Y.mat
beta.mat <- beta.op.cov %*% Y.mat
var.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))

i = 3
plot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))
lines(seq(0,100, 100), seq(0, 100, 100))

var.beta.hat <- diag(1/diag(solve(t(model.mat.cov) %*% model.mat.cov)))
t.scores <- sqrt(var.beta.hat) %*% beta.mat %*% diag(1/sqrt(var.vec))

```


##Multivariate Gaussian with Dispersed Multinomial Covariance Structure

The model I fit below assumes that for $p_i = \Omega^T x_i$,
\[
c_i \sim N\left(p_i, \Sigma_i\right), \quad \Sigma_i = \text{diag}\left(\sigma_1, \ldots, \sigma_K\right) \left(\text{diag}\left( p_i \right) - p_i p_i^T\right) \text{diag}\left(\sigma_1, \ldots, \sigma_K\right)
\]

First, I will get an estimate of $\sigma_k^2$ using the above simple univariate regressions. This will be used as a starting point in the optimization procedure. If $v_k$ is the variance in the OLS regression performed above, then $\sigma_k^2 = \frac{v_k}{p(1-p)}$, where $p$ is the cell type proportion. We need this to be small for our optimization/downstream method to perform well.
```{r Estimate sigma2}
var.OLS <- var.vec/1e4    #Michelle's proportions were given in %
ave.hat <- apply(Y.mat.hat, 2, mean)/100   #Average predicted proportion = average measured cell type proportion across individuals
sigma2.vec <- var.OLS/ave.hat/(1-ave.hat)
sigma2.0 <- median(sigma2.vec)   #Starting value of sigma^2 for optimization procedure. It corresponds to about 38 observations (a little small, although enough to make the ).
```

Since our naive estimated $\sigma^2$ is relatively large compared to the measured B cells and monocytes are relatively small (~1% and 0.5%, respectively), we do not have the power to accurately estimate their means. Thereore, we should consider combining them into the 'other' category. Eosinophil's are right on the border of what we have the power to estimate (note that these have been linked to the devlopment of asthma, so it may be crucial that we include them).

In this next part, I will get starting values for the mean. I will pool B cells and Monocytes into the 'Other' category.
```{r Get Initial Estiamats for the mean}
pool.cells <- c('Bcell.act')
C <- Y.mat[,! colnames(Y.mat) %in% pool.cells]
C[,which(colnames(C)=='Other.act')] = C[,which(colnames(C)=='Other.act')] + apply(cbind(Y.mat[,colnames(Y.mat) %in% pool.cells]), 1, sum)
C <- C[,-which(colnames(C)=='Other.act')]/100   #C is cell type matrix to be used in optimization
Omega.0 <- beta.mat[,! colnames(Y.mat) %in% pool.cells]/100
Omega.0 <- Omega.0[,-which(colnames(Omega.0)=='Other.act')]

age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
X <- as.matrix(model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS))
```

Optimize the likelihood using a Newton line-search with $K$ dispersion parameters
```{r Optimize Likelihood}
n <- nrow(X)
d <- ncol(X)
K <- ncol(C)
grad.tol <- 1e-10   #Gradient tolerance
ML <- MaxLike.NewtonLS_Ksigma(X, C, Omega.0, rep(sigma2.0^0.5,K), grad.tol)
Omega <- ML$Omega    #A covariate x cell type matrix
FI <- ML$I     #Fisher information matrix at optimum
cov.theta <- solve(FI)
sigma <- ML$sigma    #sigma (i.e. #multinomial observations ~ 1/sigma^2)
mean.cell <- X %*% Omega
stand.Omega <- Omega/matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)
sd.Omega <- matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)
colnames(sd.Omega) <- colnames(Omega)
rownames(sd.Omega) <- rownames(Omega)

tmp.sigma <- matrix(sigma, nrow=1, ncol=K)
colnames(tmp.sigma) <- paste0('sigma_', as.character((1:K)))
tmp.pred.sigma <- rbind(Omega[1,])/(sqrt(rbind(Omega[1,]*(1-Omega[1,]))) * tmp.sigma)
knitr::kable(tmp.sigma, digits=4, caption="Estimates for sigma's", longtable=F)
knitr::kable(tmp.pred.sigma, digits=4, caption="Standardized Intercepts (i.e. how accurate is our prediction)", longtable=F)
knitr::kable(head(Omega.0), digits = 4, caption = "Omega using Univariate OLS", longtable=F)
knitr::kable(head(Omega), digits = 4, caption = "Omega using Multivariate Maximum Likelihood", longtable=F)
t.scores.table <- t.scores[,! colnames(Y.mat) %in% c('Other.act', pool.cells)]
colnames(t.scores.table) <- colnames(Omega)
rownames(t.scores.table) <- rownames(Omega)
knitr::kable(head(sd.Omega), digits = 4, caption = "SD of Estimated Effects using Fisher Information of ML Estimate", longtable=F)
knitr::kable(head(stand.Omega), digits = 4, caption = "Standardized Effects using Fisher Information of ML Estimate", longtable=F)
```

##Are our data overdispersed?
While the estimate $\hat{\sigma_k}^2_{\text{ML}}$ is the maximum likelihood estimate, I want to make sure the variance of the residuals is not is not larger than we would expect from our ML model (this should not be the case, as our estimated parameters make the data the most 'normal'). Let $\hat{\mu}_i = \hat{\Omega}_{\text{ML}}^T x_i$, $S_i^T S_i = \hat{\Sigma}_i = \hat{\sigma}^2_{\text{ML}}\left[\text{diag}\left( \hat{\mu}_i \right) - \hat{\mu}_i \hat{\mu}_i^T \right]$ and the residual for individual $i$ be $\hat{r}_i = c_i - \hat{\mu}_i$. Then $S_i^{-T} \hat{r}_i$ is approximately $N(0,1)$. Using standard multivariate regression theory, we can estimate the dispersion as
\[
\text{disp} = \left(\frac{\text{det}\left( \frac{1}{n-K}\hat{R}^T \hat{R}\right)}{E\det(W_K)}\right)^{1/K}
\]
where $W_K \sim \mathcal{W}_K \left( I_K, n-d\right)$. We (obviously) find that there is no dispersion with our estimated parameters.

```{r Estimate Overdispersion}
Resid <- C - mean.cell
Stand.Resid <- C - mean.cell
for (i in 1:n) {
  p.i <- mean.cell[i,]
  Sigma.i <- diag(sigma) %*% (diag(p.i) - cbind(p.i) %*% rbind(p.i)) %*% diag(sigma)
  R.i <- chol(Sigma.i)   #R.i'R.i = Sigma.i
  Stand.Resid[i,] = as.vector( solve(t(R.i), Resid[i,]) )   #Standardized residuals should be iid N(0,1)
}

##Look at distribution of det(Wishart) to get an idea how large Stand.Resid are##
#Note E|W| = n^{-p} * n * ... * (n - p + 1), where W ~ Wishart_p(I_p, n), since n^p|W| ~ \chi^2_n * ... \chi^2_{n-p+1}, where the \chi^2 are independent
precision.wish <- n
mean.detW <- exp( sum(log( (precision.wish:(precision.wish - K + 1)) )) ) * precision.wish^(-K)
disp <- (det(t(Stand.Resid) %*% Stand.Resid/precision.wish) / mean.detW)^(1/K)
#sigma2.final <- sigma2 * disp
#Note that the minimum and maximum eigen values are well within the range of a random Wishart matrix with dimension K and n observations. This does not validate the method, though. It just means we are not too wrong.
```

Using the Fisher information at our estimated parameters as an estimate of their covariance, we come to the same conclusion about their significance using the univariate regression (the Fisher information includes the estimate for $\sigma_k^2$). I think this is a feasible method to estimate cell-type, provided we have enough individuals. The only problem is that for Eos.act, $\hat{\sigma_k}_{\text{ML}} = 0.14$, meaning our predictions will protrude through the simplex 93% of the time...

We might be able to use this as a rough estimate for the variance of our predictions. That way, we can determine how much individuals with unobserved cell types will help in our estimation.

##Probit-transform of the data
The only statistically significant predictors in the univariate regression are the intercept in all regressions (obviously) and the effect on age in 'other' cells. Maybe we can completely ignore the fact that these depend on covariates? They do not appear to depend on them...

cell types are probit-transformed:

```{r}
age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
model.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
H.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
Y.mat <- qnorm(as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])/100)
Y.mat.hat <- H.cov %*% Y.mat
beta.mat <- beta.op.cov %*% Y.mat
var.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))

i = 3
plot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))
lines(seq(0,100, 100), seq(0, 100, 100))
```

The conclusion is that regardless of scale (probit-transformed or regular scale), the current covariates (age, sex, asthma status) are a poor predictor of cell type. It also looks like we can get away with binning ages.

## Is logit(beta value) an appropriate scale to do regression against cell type (hopefully yes, so we can assume a normal response)

Load Michele's data

```{r}
#Load SWAN-normalized data
#Variables are Mset.swan.norm and rgset
load(file="/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/MyWork/SavedObjects/Rfile.RData")
meth.norm <- getMeth(Mset.swan.norm)
unmeth.norm <- getUnmeth(Mset.swan.norm)
beta.values <- meth.norm/(meth.norm+unmeth.norm+100)  #These are ordered by row in data.cov
```

Identify columns of the data that correspond to the data that I have in data.LPS (i.e. cell type data) and perform a regression

```{r}
col.use <- match(ind.ids.cell, ind.ids.covar)
beta.cell <- beta.values[,col.use]
M.cell <- log(beta.cell/(1-beta.cell))
model.mat <- model.matrix(~Tcell.act + Bcell.act + Eos.act + Neut.act + Mono.act, data=data.LPS)
beta.op <- solve(t(model.mat) %*% model.mat, t(model.mat))
H <- model.mat %*% beta.op
H.perp <- diag(1,ncol(H)) - H

coef.beta <- beta.op %*% t(beta.cell)
coef.M <- beta.op %*% t(M.cell)

SSreg.beta <- apply(beta.cell %*% H, 1, var)
SSreg.M <- apply(M.cell %*% H, 1, var)
SSY.beta <- apply(beta.cell, 1, var)
SSY.M <- apply(M.cell, 1, var)

R2.beta <- SSreg.beta/SSY.beta
R2.M <- SSreg.M/SSY.M

plot(R2.beta, R2.M, pch=".", xlim=range(R2.M, R2.beta), ylim=range(R2.M, R2.beta), main='Comparison of Variance Explained by Cell type', xlab='R^2 when Y = beta value', ylab='R^2 when Y = M value')
lines(seq(0,1,length=100), seq(0,1,length=100), col="red")
```

From this, it looks like we can use M-values when dealing with confounding due to cell type. It also appears cell type is NOT very dependent on these coefficients, but they may be dependent in other data sets.

## How well does Houseman's method do in predicting cell type?

I now want to look at how well Houseman's method perform's in predicting cell type in comparison to using training data. If it does well, there is no need to use training data. We can just use Houseman's trainin data.

How Tcell prediction differs between Houseman's 2012 method and a simple regression using Hutterite cell proportions (I added Houseman's CD4 and CD8 estimates to get his 'Tcell' proportion)
```{r}
House.cell <- data.LPS[(ncol(data.LPS)-12):(ncol(data.LPS)-7)]
Tcell.house <- House.cell[,1] + House.cell[,2]
Tcell.act <- data.LPS$Tcell.act
risk.house <- sum((Tcell.house*100 - Tcell.act)^2)/length(Tcell.act)

Tcell.hat <- H.cov %*% cbind(Tcell.act)
LOO.cv <- sqrt(mean( (Tcell.act - Tcell.hat)^2/(1 - diag(H.cov))^2 ))
print(paste0('Average L2 Loss for Houseman Tcells = ', as.character(signif(risk.house, digits=4))))
print(paste0('Average LOO CV L2 Loss for Regression Tcells = ', as.character(signif(LOO.cv, digits=4))))
```

Now with Bcells:
```{r}
Bcell.house <- House.cell$Bcell
Bcell.act <- data.LPS$Bcell.act
B.risk.house <- sum((Bcell.house*100 - Bcell.act)^2)/length(Bcell.act)

Bcell.hat <- H.cov %*% cbind(Bcell.act)
B.LOO.cv <- sqrt(mean( (Bcell.act - Bcell.hat)^2/(1 - diag(H.cov))^2 ))
print(paste0('Average L2 Loss for Houseman Bcells = ', as.character(signif(B.risk.house, digits=4))))
print(paste0('Average LOO CV L2 Loss for Regression Bcells = ', as.character(signif(B.LOO.cv, digits=4))))
```

## Conclusion

1. Houseman's training data does a poor job in predicting cell type in comparison to a simple linear regression with cell proportion training data.

2. At least in the Hutterite data set, cell proportion looks to be independent of asthma status, age (all individuals were 7-14 years old) and gender. They may be more dependent on covariates for different data sets, however.

3. We can estimate cell proportions using a Gaussian model with multinomial likelihood. With enough individuals we can get a good estimate of $\hat{\Omega}$. However, we estimate $\sigma \approx 0.18$, meaning our prediction may portrude through the simplex (see standardized effects table above).

4. We still need to understand how much additional information we can get from individuals with unmeasured cell types to estimate main effects of interest.


##How Much Additional Information do we get form Individuals with Unobserved Cell Types?
We suppose we have two groups of individuals, one with observed cell types and one without cell types. We model both these groups as
\[
Y_1 = B_{p \times d}X_1 + L_{p \times k}C_1^T + E_1, \quad Y_1 \in \mathbb{R}^{p \times n_1}
\]
\[
Y_2 = BX_2 + L\Omega_{K \times d}^T X_2 + L\Xi_{K \times n_2} + E_2, \quad Y_2 \in \mathbb{R}^{p \times n_2}.
\]
We assume that the columns of $E_i$ are indpendent. For now, $\Xi_{K \times n_2} = \left( \xi_1 \cdots \xi_{n_2} \right)$, where $\xi_i \sim N_K\left( 0,\Sigma_i \right)$, $p_i = \Omega^T_{K \times d}x_i$ and
\[
\Sigma_i = \text{diag}\left(\sigma_1, \ldots, \sigma_K\right) \left(\text{diag}\left( p_i \right) - p_i p_i^T\right) \text{diag}\left(\sigma_1, \ldots, \sigma_K\right).
\]
We first suppose that $L_{p \times K}$ with rows $\ell_g$, $g = 1, \ldots, p$ and $\Omega_{d \times K}$ are known. We are interested in determining how must additional information $Y_2$ gives us, which can be measured by how much the variance of $\hat{B}$ shrinks with the additional observations (*).

As an aside, if $n_1 = 0$, then we are back to the unsupervised scenario. I was originally modeling $\Xi$ as a $MN\left(0, I_K, I_{n_2}\right)$. However, for strong covariates in $X_2$, the vectors $\xi_i$ have non-identical covariance that depends on $X_2$, which invalidates the methodology I had proposed. If, however, there are only a few covariates (like disease vs. non-disease), then we can apply our favorite factor analysis method to get an estimate for $L$ by splitting individuals by diseased and non-diseased.

To study (*), we look at a very simple example. We consider a scenario when the only covariate of interest is disease status (i.e. yes or no covariates). For site $g$, let $\beta_g$ be the $g^{\text{th}}$ row of $B$. Our model is then
\[
Y_{1_g} = X_1 \beta_g + C_1 \ell_g + \epsilon_{1_g}
\]
\[
Y_{2_g} = X_2 \beta_g + X_2 \Omega_{d \times K} \ell_g + \Xi^T_{n_2 \times K} \ell_g + \epsilon_{2_g}
\]
where $\epsilon_{i_g} \sim N\left(0, \sigma_g^2 I_{n_i}\right)$ and the columns of $\Xi_{K \times n_2}$, $\xi_i$, are distributed as above (with covariance $\Sigma_i$ that depends on $X_2$). If $\Omega$ and $L$ are known, then the variance for individual $i$ in group 1 is $\sigma_g^2$ and if $i$ is in group 2 it is $\sigma_g^2 + \ell_g^T \Sigma_i \ell_g$. Note that individuals remain independent when $\Omega$ and $L$ are known. For simplicity, we assume there are an equal number of diseased and non-diseased individuals in each group. Let $S_j = \text{var}\left(Y_j\right)$, $j = 1,2$. Then
\[
\text{var}\left(\hat{\beta}_g\right) = \left( X_1^T S_1^{-1}X_1 + X_2^T S_2^{-1}X_2 \right)^{-1} = \text{diag}\left( \frac{n_1}{2}\sigma_g^{-2} + \sum\limits_{i=1}^{n_2/2}\left( \sigma_g^2 + \ell_g^T \Sigma_i \ell_g\right)^{-1}, \frac{n_1}{2}\sigma_g^{-2} + \sum\limits_{i=n_2/2}^{n_2}\left( \sigma_g^2 + \ell_g^T \Sigma_i \ell_g\right)^{-1}\right)^{-1}
\]
\[
= \text{diag}\left( \frac{n_1}{2}\sigma_g^{-2} + \frac{n_2}{2}\ell_g^T \Sigma_{\text{Asthma}}\ell_g, \frac{n_1}{2}\sigma_g^{-2} + \frac{n_2}{2}\ell_g^T \Sigma_{\text{No Asthma}}\ell_g \right)^{-1}
\]
Without the individuals from $Y_2$,
\[
\text{var}\left(\hat{\beta}_g\right) = \text{diag}\left( \frac{2}{n_1}\sigma_g^2, \frac{2}{n_1}\sigma_g^2 \right).
\]
Therefore, the reduction of the variance of treatment difference is exactly
\[
\frac{1}{2}\left( \frac{1}{1 + \frac{n_2}{n_1} \frac{\sigma_g^2}{\alpha_{\text{Asthma}}}} + \frac{1}{1 + \frac{n_2}{n_1} \frac{\sigma_g^2}{\alpha_{\text{No Asthma}}}} \right)
\]
where $\alpha_{k} = \sigma_g^2 + \ell_g^T \Sigma_k \ell_g$.

Below, I will estimate $L$ and $\sigma_g^2$ from Michelle's data and use those estimates to determine the reduction of variance.

```{r Determine L and Variance}
m <- nrow(M.cell)
X.L <- model.matrix(~Tcell.act + Eos.act + Neut.act + Mono.act + asthma + sex + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.L <- solve(t(X.L) %*% X.L, t(X.L))
H.L <- X.L %*% solve(t(X.L) %*% X.L, t(X.L))
perp.L <- diag(n) - H.L
beta.L <- M.cell %*% t(beta.op.L)
L <- beta.L[,2:5]    #In the orderof Tcell, Eosinophil, Neutrophil, Monocyte
LO <- L %*% t(Omega)     #This is the potion of the effect due to cell type, i.e. Y = (B + LO)X + E. We want to determine how large B is in relation ot LO. If B is large, then cell type does NOT contribute to confounding
B <- beta.L[,6:8]
var.g.vec <- rep(0, m)     #An estimate of sigma^2_g for each site g = 1,...,p
for (g in 1:m) {
  tmp.site <- cbind(M.cell[g,])
  var.g.vec[g] <- as.numeric(t(tmp.site) %*% perp.L %*% tmp.site / (n - ncol(X.L)))
}
x.asthma <- cbind(c(1,0,1,0))
x.nasthma <- cbind(c(1,0,0,0))
p.asthma <- t(Omega) %*% x.asthma
p.nasthma <- t(Omega) %*% x.nasthma
Sigma.asthma <- diag(sigma) %*% (diag(as.vector(p.asthma)) - p.asthma %*% t(p.asthma)) %*% diag(sigma)
Sigma.nasthma <- diag(sigma) %*% (diag(as.vector(p.nasthma)) - p.nasthma %*% t(p.nasthma)) %*% diag(sigma)

var.cell.asthma <- rep(0,m)    #l' Sigma_asthma l
var.cell.nasthma <- rep(0,m)    #l' Sigma_noasthma l
for (g in 1:m) {
  l.g <- cbind(L[g,])
  var.cell.asthma[g] <- t(l.g) %*% Sigma.asthma %*% l.g
  var.cell.nasthma[g] <- t(l.g) %*% Sigma.nasthma %*% l.g
}

hist(var.g.vec/(var.g.vec + var.cell.asthma), main="Relative Variance of Residuals", ylab="Frequency", xlab="sigma_g^2/(sigma_g^2 + alpha_asthma)")
```

We conlcude that in THIS data set, the error due to cell type is trivial in comparison to the residual error in our model.

##Determine Proportion of Effect Due to Cell Type
Recall that in our basice model for methylation,
\[
Y_{p \times n} = \left( B + L\Omega^T \right)X + \tilde{E}_{p \times n}.
\]
Since we have estimates for $B, L$ and $\Omega$, we can look at the proportion of of the effect due to $B$ and $L$ and use the metric $\frac{\Vert \ell_g \Vert}{\Vert \ell_g + \beta_g \Vert}$. If this is small, we can conclude that cell type is NOT confounding our coefficient estimates (for these data).

```{r Proportion of Effect}
prop.effect <- rep(0, m)
for (g in 1:m) {
  prop.effect[g] <- sqrt( sum(LO[g,2:ncol(LO)] * LO[g,2:ncol(LO)])/sum( c(LO[g,2:ncol(LO)], B[g,])*c(LO[g,2:ncol(LO)], B[g,]) ) )
}
hist(prop.effect, breaks=90, main="Propotion of Effect Explained by Cell Type", xlab="Proportion of Effect Explained by Cell Type", ylab="Frequency")
```

##Conclusions
1.) We have a probability model to model cell types in individuals. This model has many practical benefits, since the we assume cell proportions are normally distributed with covariance structure related to the multinomial covariance structure. However, this model suffers from a few drawbacks:

&ensp;&ensp;&ensp;i.) For the above estimated covariance structure, our prediction is not gauranteed to lie in the simplex (the smallest marginal probability of it lieing outside is ~0.09).

&ensp;&ensp;&ensp;ii.) We incorrectly assume that the residuals are symmetric for points with mean close to the boundary of the simplex.

2.) Whether or not our covariates determine cell type is very dependent on the data. In Michelle's Hutterite data, we show that covariates are NOT contaminated by cell type and that error due to cell type is negligible (when $L$ and $\Omega$ are known).

Based on this, I would argure a cell type training set is most appropriate to performa analysis. I think we lose too much information and make overly-conservative assumptions in an unsupervised approach.

## Session information

```{r info}
sessionInfo()
```
