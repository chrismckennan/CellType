---
title: "Investigate cell type and how it relates to covariates"
author: "Chris McKennan"
date: 2016-02-05
---

**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=T}
source("chunk-options.R")
source("../R/OptimizeLogLike.R")
source("../R/OptimizeLogLike_Ksigma.R")
```

The purpose of this file is to investigate how cell type is related to the covariates. That is, do we have any hope in predicting cell type given an individuals covariate information. In order to apply our supervised method, we need the variance in cell type among unrelated individuals to be small, given the covariate information.

## Load required functions

```{r}
library(minfi)
library('IlluminaHumanMethylation450kmanifest')
library('IlluminaHumanMethylation450kanno.ilmn12.hg19')
library('FlowSorted.Blood.450k')      ##Methylation data on 6 males, 10 cell types; RGset object
library('RefFreeEWAS')
library('nlme')
library('corpcor')
library('sva')
library('knitr')
library('printr')
```


## Get data into R

Get the data into R and initialize global variables.

```{r}
path.LPS.kids <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/HTkids_cellprop/HTkids_LPS_covar_ImpAct_10115.txt'
path.methylation <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/Hutterite_Methylation_data/Meth_covar_allsamp_impcells_10115.txt'
data.cov <- data.frame(read.table(path.methylation, sep="\t", quote="\"", dec=".", header=T, check.names=F))
data.LPS <- data.frame(read.table(path.LPS.kids, sep="\t", quote="\"", dec=".", header=T, check.names=F))
cell.names <- colnames(data.LPS)[(ncol(data.LPS) - 5):ncol(data.LPS)]
n.cells <- length(cell.names)
n.ind <- nrow(data.LPS)
ind.ids.cell <- data.LPS$Rowid    #IDs of individuals with cell type data
ind.ids.covar <- data.cov$Rowid   #IDs of individuals in complete covariate file
```

## How well do covariates predict cell type
Cell types are on regular 0-1 scale

```{r}
age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
model.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
H.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
Y.mat <- as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])
Y.mat.hat <- H.cov %*% Y.mat
beta.mat <- beta.op.cov %*% Y.mat
var.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))

i = 3
plot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))
lines(seq(0,100, 100), seq(0, 100, 100))

var.beta.hat <- diag(1/diag(solve(t(model.mat.cov) %*% model.mat.cov)))
t.scores <- sqrt(var.beta.hat) %*% beta.mat %*% diag(1/sqrt(var.vec))

```


##Multivariate Gaussian with Dispersed Multinomial Covariance Structure

The model I fit below assumes that for $p_i = \Omega^T x_i$,
\[
c_i \sim N\left(p_i, \Sigma_i\right), \quad \Sigma_i = \text{diag}\left(\sigma_1, \ldots, \sigma_K\right) \left(\text{diag}\left( p_i \right) - p_i p_i^T\right) \text{diag}\left(\sigma_1, \ldots, \sigma_K\right)
\]

First, I will get an estimate of $\sigma_k^2$ using the above simple univariate regressions. This will be used as a starting point in the optimization procedure. If $v_k$ is the variance in the OLS regression performed above, then $\sigma_k^2 = \frac{v_k}{p(1-p)}$, where $p$ is the cell type proportion. We need this to be small for our optimization/downstream method to perform well.
```{r Estimate sigma2}
var.OLS <- var.vec/1e4    #Michelle's proportions were given in %
ave.hat <- apply(Y.mat.hat, 2, mean)/100   #Average predicted proportion = average measured cell type proportion across individuals
sigma2.vec <- var.OLS/ave.hat/(1-ave.hat)
sigma2.0 <- median(sigma2.vec)   #Starting value of sigma^2 for optimization procedure. It corresponds to about 38 observations (a little small, although enough to make the ).
```

Since our naive estimated $\sigma^2$ is relatively large compared to the measured B cells and monocytes are relatively small (~1% and 0.5%, respectively), we do not have the power to accurately estimate their means. Thereore, we should consider combining them into the 'other' category. Eosinophil's are right on the border of what we have the power to estimate (note that these have been linked to the devlopment of asthma, so it may be crucial that we include them).

In this next part, I will get starting values for the mean. I will pool B cells and Monocytes into the 'Other' category.
```{r Get Initial Estiamats for the mean}
pool.cells <- c('Bcell.act')
C <- Y.mat[,! colnames(Y.mat) %in% pool.cells]
C[,which(colnames(C)=='Other.act')] = C[,which(colnames(C)=='Other.act')] + apply(cbind(Y.mat[,colnames(Y.mat) %in% pool.cells]), 1, sum)
C <- C[,-which(colnames(C)=='Other.act')]/100   #C is cell type matrix to be used in optimization
Omega.0 <- beta.mat[,! colnames(Y.mat) %in% pool.cells]/100
Omega.0 <- Omega.0[,-which(colnames(Omega.0)=='Other.act')]

age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
X <- as.matrix(model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS))
```

Optimize the likelihood using a Newton line-search with $K$ dispersion parameters
```{r Optimize Likelihood}
n <- nrow(X)
d <- ncol(X)
K <- ncol(C)
grad.tol <- 1e-10   #Gradient tolerance
ML <- MaxLike.NewtonLS_Ksigma(X, C, Omega.0, rep(sigma2.0^0.5,K), grad.tol)
Omega <- ML$Omega    #A covariate x cell type matrix
FI <- ML$I     #Fisher information matrix at optimum
cov.theta <- solve(FI)
Hess <- ML$Hessian
sigma <- ML$sigma    #sigma (i.e. #multinomial observations ~ 1/sigma^2)
mean.cell <- X %*% Omega
stand.Omega <- Omega/matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)
sd.Omega <- matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)
colnames(sd.Omega) <- colnames(Omega)
rownames(sd.Omega) <- rownames(Omega)

tmp.sigma <- matrix(sigma, nrow=1, ncol=K)
colnames(tmp.sigma) <- paste0('sigma_', as.character((1:K)))
tmp.pred.sigma <- rbind(Omega[1,])/(sqrt(rbind(Omega[1,]*(1-Omega[1,]))) * tmp.sigma)
knitr::kable(tmp.sigma, digits=4, caption="Estimates for sigma's", longtable=F)
knitr::kable(tmp.pred.sigma, digits=4, caption="Standardized Intercepts (i.e. how accurate is our prediction)", longtable=F)
knitr::kable(head(Omega.0), digits = 4, caption = "Omega using Univariate OLS", longtable=F)
knitr::kable(head(Omega), digits = 4, caption = "Omega using Multivariate Maximum Likelihood", longtable=F)
t.scores.table <- t.scores[,! colnames(Y.mat) %in% c('Other.act', pool.cells)]
colnames(t.scores.table) <- colnames(Omega)
rownames(t.scores.table) <- rownames(Omega)
knitr::kable(head(sd.Omega), digits = 4, caption = "SD of Estimated Effects using Fisher Information of ML Estimate", longtable=F)
knitr::kable(head(stand.Omega), digits = 4, caption = "Standardized Effects using Fisher Information of ML Estimate", longtable=F)
```

##Are our data overdispersed?
While the estimate $\hat{\sigma_k}^2_{\text{ML}}$ is the maximum likelihood estimate, I want to make sure the variance of the residuals is not is not larger than we would expect from our ML model (this should not be the case, as our estimated parameters make the data the most 'normal'). Let $\hat{\mu}_i = \hat{\Omega}_{\text{ML}}^T x_i$, $S_i^T S_i = \hat{\Sigma}_i = \hat{\sigma}^2_{\text{ML}}\left[\text{diag}\left( \hat{\mu}_i \right) - \hat{\mu}_i \hat{\mu}_i^T \right]$ and the residual for individual $i$ be $\hat{r}_i = c_i - \hat{\mu}_i$. Then $S_i^{-T} \hat{r}_i$ is approximately $N(0,1)$. Using standard multivariate regression theory, we can estimate the dispersion as
\[
\text{disp} = \left(\frac{\text{det}\left( \frac{1}{n-K}\hat{R}^T \hat{R}\right)}{E\det(W_K)}\right)^{1/K}
\]
where $W_K \sim \mathcal{W}_K \left( I_K, n-d\right)$. We (obviously) find that there is no dispersion with our estimated parameters.

```{r Estimate Overdispersion}
Resid <- C - mean.cell
Stand.Resid <- C - mean.cell
for (i in 1:n) {
  p.i <- mean.cell[i,]
  Sigma.i <- diag(sigma) %*% (diag(p.i) - cbind(p.i) %*% rbind(p.i)) %*% diag(sigma)
  R.i <- chol(Sigma.i)   #R.i'R.i = Sigma.i
  Stand.Resid[i,] = as.vector( solve(t(R.i), Resid[i,]) )   #Standardized residuals should be iid N(0,1)
}

##Look at distribution of det(Wishart) to get an idea how large Stand.Resid are##
#Note E|W| = n^{-p} * n * ... * (n - p + 1), where W ~ Wishart_p(I_p, n), since n^p|W| ~ \chi^2_n * ... \chi^2_{n-p+1}, where the \chi^2 are independent
precision.wish <- n
mean.detW <- exp( sum(log( (precision.wish:(precision.wish - K + 1)) )) ) * precision.wish^(-K)
disp <- (det(t(Stand.Resid) %*% Stand.Resid/precision.wish) / mean.detW)^(1/K)
#sigma2.final <- sigma2 * disp
#Note that the minimum and maximum eigen values are well within the range of a random Wishart matrix with dimension K and n observations. This does not validate the method, though. It just means we are not too wrong.
```

Using the Fisher information at our estimated parameters as an estimate of their covariance, we come to the same conclusion about their significance using the univariate regression (the Fisher information includes the estimate for $\sigma_k^2$). I think this is a feasible method to estimate cell-type, provided we have enough individuals. The only problem is that for Eos.act, $\hat{\sigma_k}_{\text{ML}} = 0.14$, meaning our predictions will protrude through the simplex 93% of the time...

We might be able to use this as a rough estimate for the variance of our predictions. That way, we can determine how much individuals with unobserved cell types will help in our estimation.

##Probit-transform of the data
The only statistically significant predictors in the univariate regression are the intercept in all regressions (obviously) and the effect on age in 'other' cells. Maybe we can completely ignore the fact that these depend on covariates? They do not appear to depend on them...

cell types are probit-transformed:

```{r}
age.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)
model.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
H.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))
Y.mat <- qnorm(as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])/100)
Y.mat.hat <- H.cov %*% Y.mat
beta.mat <- beta.op.cov %*% Y.mat
var.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))

i = 3
plot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))
lines(seq(0,100, 100), seq(0, 100, 100))
```

The conclusion is that regardless of scale (probit-transformed or regular scale), the current covariates (age, sex, asthma status) are a poor predictor of cell type. It also looks like we can get away with binning ages.

## Is logit(beta value) an appropriate scale to do regression against cell type (hopefully yes, so we can assume a normal response)

Load Michele's data

```{r}
#Load SWAN-normalized data
#Variables are Mset.swan.norm and rgset
load(file="/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/MyWork/SavedObjects/Rfile.RData")
meth.norm <- getMeth(Mset.swan.norm)
unmeth.norm <- getUnmeth(Mset.swan.norm)
beta.values <- meth.norm/(meth.norm+unmeth.norm+100)  #These are ordered by row in data.cov
```

Identify columns of the data that correspond to the data that I have in data.LPS (i.e. cell type data) and perform a regression

```{r}
col.use <- match(ind.ids.cell, ind.ids.covar)
beta.cell <- beta.values[,col.use]
M.cell <- log(beta.cell/(1-beta.cell))
model.mat <- model.matrix(~Tcell.act + Bcell.act + Eos.act + Neut.act + Mono.act, data=data.LPS)
beta.op <- solve(t(model.mat) %*% model.mat, t(model.mat))
H <- model.mat %*% beta.op
H.perp <- diag(1,ncol(H)) - H

coef.beta <- beta.op %*% t(beta.cell)
coef.M <- beta.op %*% t(M.cell)

SSreg.beta <- apply(beta.cell %*% H, 1, var)
SSreg.M <- apply(M.cell %*% H, 1, var)
SSY.beta <- apply(beta.cell, 1, var)
SSY.M <- apply(M.cell, 1, var)

R2.beta <- SSreg.beta/SSY.beta
R2.M <- SSreg.M/SSY.M

plot(R2.beta, R2.M, pch=".", xlim=range(R2.M, R2.beta), ylim=range(R2.M, R2.beta), main='Comparison of Variance Explained by Cell type', xlab='R^2 when Y = beta value', ylab='R^2 when Y = M value')
lines(seq(0,1,length=100), seq(0,1,length=100), col="red")
```

From this, it looks like we can use M-values when dealing with confounding due to cell type. It also appears cell type is NOT very dependent on these coefficients, but they may be dependent in other data sets.

## How well does Houseman's method do in predicting cell type?

I now want to look at how well Houseman's method perform's in predicting cell type in comparison to using training data. If it does well, there is no need to use training data. We can just use Houseman's trainin data.

How Tcell prediction differs between Houseman's 2012 method and a simple regression using Hutterite cell proportions (I added Houseman's CD4 and CD8 estimates to get his 'Tcell' proportion)
```{r}
House.cell <- data.LPS[(ncol(data.LPS)-12):(ncol(data.LPS)-7)]
Tcell.house <- House.cell[,1] + House.cell[,2]
Tcell.act <- data.LPS$Tcell.act
risk.house <- sum((Tcell.house*100 - Tcell.act)^2)/length(Tcell.act)

Tcell.hat <- H.cov %*% cbind(Tcell.act)
LOO.cv <- sqrt(mean( (Tcell.act - Tcell.hat)^2/(1 - diag(H.cov))^2 ))
print(paste0('Average L2 Loss for Houseman Tcells = ', as.character(signif(risk.house, digits=4))))
print(paste0('Average LOO CV L2 Loss for Regression Tcells = ', as.character(signif(LOO.cv, digits=4))))
```

Now with Bcells:
```{r}
Bcell.house <- House.cell$Bcell
Bcell.act <- data.LPS$Bcell.act
B.risk.house <- sum((Bcell.house*100 - Bcell.act)^2)/length(Bcell.act)

Bcell.hat <- H.cov %*% cbind(Bcell.act)
B.LOO.cv <- sqrt(mean( (Bcell.act - Bcell.hat)^2/(1 - diag(H.cov))^2 ))
print(paste0('Average L2 Loss for Houseman Bcells = ', as.character(signif(B.risk.house, digits=4))))
print(paste0('Average LOO CV L2 Loss for Regression Bcells = ', as.character(signif(B.LOO.cv, digits=4))))
```

## Conclusion

1. Houseman's training data does a poor job in predicting cell type in comparison to a simple linear regression with cell proportion training data.

2. At least in the Hutterite data set, cell proportion looks to be independent of asthma status, age (all individuals were 7-14 years old) and gender. They may be more dependent on covariates for different data sets, however.

3. We can estimate cell proportions using a Gaussian model with multinomial likelihood. With enough individuals we can get a good estimate of $\hat{\Omega}$. However, we estimate $\sigma \approx 0.18$, meaning our prediction may portrude through the simplex (see standardized effects table above).

4. We still need to understand how much additional information we can get from individuals with unmeasured cell types to estimate main effects of interest.


##How Much Additional Information do we get form Individuals with Unobserved Cell Types?
We suppose we have two groups of individuals, one with observed cell types and one without cell types. We model both these groups as
\[
Y_1 = B_{p \times d}X_1 + L_{p \times k}C_1^T + E_1, \quad Y_1 \in \mathbb{R}^{p \times n_1}
\]
\[
Y_2 = BX_2 + L\Omega_{K \times d}^T X_2 + L\Xi_{K \times n_2} + E_2, \quad Y_2 \in \mathbb{R}^{p \times n_2}.
\]
We assume that the columns of $E_i$ are indpendent. For now, $\Xi_{K \times n_2} = \left( \xi_1 \cdots \xi_{n_2} \right)$, where $\xi_i \sim N_K\left( 0,\Sigma_i \right)$, $p_i = \Omega^T_{K \times d}x_i$ and
\[
\Sigma_i = \text{diag}\left(\sigma_1, \ldots, \sigma_K\right) \left(\text{diag}\left( p_i \right) - p_i p_i^T\right) \text{diag}\left(\sigma_1, \ldots, \sigma_K\right).
\]
We first suppose that $L_{p \times K}$ with rows $\ell_g$, $g = 1, \ldots, p$ and $\Omega_{d \times K}$ are known. We are interested in determining how must additional information $Y_2$ gives us, which can be measured by how much the variance of $\hat{B}$ shrinks with the additional observations (*).

As an aside, if $n_1 = 0$, then we are back to the unsupervised scenario. I was originally modeling $\Xi$ as a $MN\left(0, I_K, I_{n_2}\right)$. However, for strong covariates in $X_2$, the vectors $\xi_i$ have non-identical covariance that depends on $X_2$, which invalidates the methodology I had proposed. If, however, there are only a few covariates (like disease vs. non-disease), then we can apply our favorite factor analysis method to get an estimate for $L$ by splitting individuals by diseased and non-diseased.

To study (*), we look at a very simple example. We consider a scenario when the only covariate of interest is disease status (i.e. yes or no covariates). For site $g$, let $\beta_g$ be the $g^{\text{th}}$ row of $B$. Our model is then
\[
Y_{1_g} = X_1 \beta_g + C_1 \ell_g + \epsilon_{1_g}
\]
\[
Y_{2_g} = X_2 \beta_g + X_2 \Omega_{d \times K} \ell_g + \Xi^T_{n_2 \times K} \ell_g + \epsilon_{2_g}
\]
where $\epsilon_{i_g} \sim N\left(0, \sigma_g^2 I_{n_i}\right)$ and the columns of $\Xi_{K \times n_2}$, $\xi_i$, are distributed as above (with covariance $\Sigma_i$ that depends on $X_2$). If $\Omega$ and $L$ are known, then the variance for individual $i$ in group 1 is $\sigma_g^2$ and if $i$ is in group 2 it is $\sigma_g^2 + \ell_g^T \Sigma_i \ell_g$. Note that individuals remain independent when $\Omega$ and $L$ are known. For simplicity, we assume there are an equal number of diseased and non-diseased individuals in each group. Let $S_j = \text{var}\left(Y_j\right)$, $j = 1,2$. Then
\[
\text{var}\left(\hat{\beta}_g\right) = \left( X_1^T S_1^{-1}X_1 + X_2^T S_2^{-1}X_2 \right)^{-1} = \text{diag}\left( \frac{n_1}{2}\sigma_g^{-2} + \sum\limits_{i=1}^{n_2/2}\left( \sigma_g^2 + \ell_g^T \Sigma_i \ell_g\right)^{-1}, \frac{n_1}{2}\sigma_g^{-2} + \sum\limits_{i=n_2/2}^{n_2}\left( \sigma_g^2 + \ell_g^T \Sigma_i \ell_g\right)^{-1}\right)^{-1}
\]
\[
= \text{diag}\left( \frac{n_1}{2}\sigma_g^{-2} + \frac{n_2}{2}\ell_g^T \Sigma_{\text{Asthma}}\ell_g, \frac{n_1}{2}\sigma_g^{-2} + \frac{n_2}{2}\ell_g^T \Sigma_{\text{No Asthma}}\ell_g \right)^{-1}
\]
Without the individuals from $Y_2$,
\[
\text{var}\left(\hat{\beta}_g\right) = \text{diag}\left( \frac{2}{n_1}\sigma_g^2, \frac{2}{n_1}\sigma_g^2 \right).
\]
Therefore, the reduction of the variance of treatment difference is exactly
\[
\frac{1}{2}\left( \frac{1}{1 + \frac{n_2}{n_1} \frac{\sigma_g^2}{\alpha_{\text{Asthma}}}} + \frac{1}{1 + \frac{n_2}{n_1} \frac{\sigma_g^2}{\alpha_{\text{No Asthma}}}} \right)
\]
where $\alpha_{k} = \sigma_g^2 + \ell_g^T \Sigma_k \ell_g$.

Below, I will estimate $L$ and $\sigma_g^2$ from Michelle's data and use those estimates to determine the reduction of variance.

```{r Determine L and Variance}
m <- nrow(M.cell)
X.L <- model.matrix(~Tcell.act + Eos.act + Neut.act + Mono.act + asthma + sex + as.numeric(Age > age.cutoff), data=data.LPS)
beta.op.L <- solve(t(X.L) %*% X.L, t(X.L))
H.L <- X.L %*% solve(t(X.L) %*% X.L, t(X.L))
perp.L <- diag(n) - H.L
beta.L <- M.cell %*% t(beta.op.L)
L <- beta.L[,2:5]    #In the orderof Tcell, Eosinophil, Neutrophil, Monocyte
LO <- L %*% t(Omega)     #This is the potion of the effect due to cell type, i.e. Y = (B + LO)X + E. We want to determine how large B is in relation ot LO. If B is large, then cell type does NOT contribute to confounding
B <- beta.L[,6:8]
var.g.vec <- rep(0, m)     #An estimate of sigma^2_g for each site g = 1,...,p
for (g in 1:m) {
  tmp.site <- cbind(M.cell[g,])
  var.g.vec[g] <- as.numeric(t(tmp.site) %*% perp.L %*% tmp.site / (n - ncol(X.L)))
}
x.asthma <- cbind(c(1,0,1,0))
x.nasthma <- cbind(c(1,0,0,0))
p.asthma <- t(Omega) %*% x.asthma
p.nasthma <- t(Omega) %*% x.nasthma
Sigma.asthma <- diag(sigma) %*% (diag(as.vector(p.asthma)) - p.asthma %*% t(p.asthma)) %*% diag(sigma)
Sigma.nasthma <- diag(sigma) %*% (diag(as.vector(p.nasthma)) - p.nasthma %*% t(p.nasthma)) %*% diag(sigma)

var.cell.asthma <- rep(0,m)    #l' Sigma_asthma l
var.cell.nasthma <- rep(0,m)    #l' Sigma_noasthma l
for (g in 1:m) {
  l.g <- cbind(L[g,])
  var.cell.asthma[g] <- t(l.g) %*% Sigma.asthma %*% l.g
  var.cell.nasthma[g] <- t(l.g) %*% Sigma.nasthma %*% l.g
}

hist(var.g.vec/(var.g.vec + var.cell.asthma), main="Relative Variance of Residuals", ylab="Frequency", xlab="sigma_g^2/(sigma_g^2 + alpha_asthma)")
```

We conlcude that in THIS data set, the error due to cell type is trivial in comparison to the residual error in our model.

##Determine Proportion of Effect Due to Cell Type
Recall that in our basice model for methylation,
\[
Y_{p \times n} = \left( B + L\Omega^T \right)X + \tilde{E}_{p \times n}.
\]
Since we have estimates for $B, L$ and $\Omega$, we can look at the proportion of of the effect due to $B$ and $L$ and use the metric $\frac{\Vert \Omega\ell_g \Vert}{\Vert \Omega\ell_g + \beta_g \Vert}$. If this is small, we can conclude that cell type is NOT confounding our coefficient estimates (for these data).

```{r Proportion of Effect}
prop.effect <- rep(0, m)
for (g in 1:m) {
  prop.effect[g] <- sqrt( sum(LO[g,2:ncol(LO)] * LO[g,2:ncol(LO)])/sum( c(LO[g,2:ncol(LO)], B[g,])*c(LO[g,2:ncol(LO)], B[g,]) ) )
}
hist(prop.effect, breaks=90, main="Propotion of Effect Explained by Cell Type", xlab="Proportion of Effect Explained by Cell Type", ylab="Frequency")
```

##Conclusions
1.) We have a probability model to model cell types in individuals. This model has many practical benefits, since the we assume cell proportions are normally distributed with covariance structure related to the multinomial covariance structure. However, this model suffers from a few drawbacks:

&ensp;&ensp;&ensp;i.) For the above estimated covariance structure, our prediction is not gauranteed to lie in the simplex (the smallest marginal probability of it lieing outside is ~0.09).

&ensp;&ensp;&ensp;ii.) We incorrectly assume that the residuals are symmetric for points with mean close to the boundary of the simplex.

2.) Whether or not our covariates determine cell type is very dependent on the data. In Michelle's Hutterite data, we show that covariates are NOT contaminated by cell type and that error due to cell type is negligible (when $L$ and $\Omega$ are known).

Based on this, I would argure a cell type training set is most appropriate to performa analysis. I think we lose too much information and make overly-conservative assumptions in an unsupervised approach.

##Investigating Test Statistics: An Unbiased OLS Estimator

In this next part I will investigate possible test statistics, given that we have cell type information on some of the individuals individuals, but not all. Suppose there are $n_1$ individuals who's cell type is known and $n_2$ with unknown cell type. Recall that the first and second moment assumptions on the predictive distribution for $C_2$, the unobserved cell types, are

1.) 
\[
E\left( C_2 \mid C_1 \right) = X_2^T \hat{\Omega}
\]
2.)
\[
\text{Var}\left( C_2^{(i)} \mid C_1 \right) = E_{\Omega} \text{Var}\left( C_2^{(i)} \mid C_1, \Omega \right) + \text{Var}_{\Omega}\left( E\left( C_2^{(i)} \mid C_1, \Omega \right) \right) = E_{\Omega}R_i\left( \Omega \right) + S_i
\]
3.)
\[
\text{Cov}\left( C_2^{(i)}, C_2^{(j)} \mid C_1 \right) = E_{\Omega} \underbrace{\text{Cov}\left( C_2^{(i)}, C_2^{(j)} \mid C_1, \Omega \right)}_{\text{0 by assumption}} + \text{Cov}_{\Omega} \left( E\left( C_2^{(i)} \mid C_1, \Omega \right), E\left( C_2^{(j)} \mid C_1, \Omega \right) \right) = S_{i \times j}
\]
where $R_i\left( \Omega \right) = \text{Var}\left( C_2^{(i)} \mid C_1, \Omega \right)$ is known (it should look like the multinomial covariance). $S_i$ and $S_{i \times j}$ relate to the uncertainty in the uncertainty in our estimate $\hat{\Omega}$ and will be described below. The model is then
\[
Y_1 = B_{p \times d} X_1 + L_{p \times K}C_1^T + E_1
\]
\[
Y_2 = BX_2 + LC_2^T + E_2
\]
where $X_i \in \mathbb{R}^{d \times n_i}$, $C_1 \in \mathbb{R}^{n_1 \times K}$, $\Omega \in \mathbb{R}^{d \times K}$ and $E_i \sim MN_{p \times n_i}\left(0, \Sigma_{p \times p}, I_{n_i}\right)$. 

Next define $l\left( \Omega \right) = \log p\left( C_1 \mid \Omega, X_1 \right)$ to be the log-likelihood for $\Omega$. This can be either a Dirichlet likelihood or a normal approximation to the Dirichlet (or some other likelihood). We then define
\[
-\left(\nabla^2 l\left(\text{vec}_{\text{cols}}(\Omega)\right) \mid_{\hat{\Omega}_{\text{MLE}}}\right)^{-1} = \left[\begin{matrix}
S_{11} & \cdots & S_{1K}\\
\vdots & \ddots & \vdots\\
S_{K1} & \cdots & S_{KK}
\end{matrix}\right],
\]
\[
S_i = \left[\begin{matrix}
x_i^T S_{11} x_i & \cdots & x_i^T S_{1K} x_i\\
\vdots & \ddots & \vdots\\
x_i^T S_{K1} x_i & \cdots & x_i^T S_{KK} x_i
\end{matrix}\right]
\]
and
\[
S_{i \times j} = \left[\begin{matrix}
x_i^T S_{11} x_j & \cdots & x_i^T S_{1K} x_j\\
\vdots & \ddots & \vdots\\
x_i^T S_{K1} x_j & \cdots & x_i^T S_{KK} x_j
\end{matrix}\right]
\]

In the estimation procedure, we want to CONDITION ON $C_1$, since these cell type data will presumably be from individuals with methylation data (that we obviously want to utilize). The estimation procedure would then be:

1.) Use $C_1$ to estimate $\Omega$ via maximum likelihood using the log-likelihood function $l\left( \Omega \right)$.

Call the maximum likelihood estimate $\hat{\Omega}_{\text{MLE}}$ (code for this procedure has been implemented and tested for the multivariate normal case). Note that since we are conditioning on $C_1$, we cannot use the Fisher information matrix as a measure of our uncertainty of $\hat{\Omega}_{\text{MLE}}$. Instead, we would like to estimate the posterior distribution $p\left(\Omega \mid C_1,X_1 \right) \propto p\left( C_1 \mid \Omega,X_1\right)p(\Omega \mid X_1)$. Since $\Omega \mid X_1$ is constrained to the interior of the simplex, our prior on $\Omega \mid X_1$ is uniform over the interior of the simplex and 0 outside of it. Therefore,
\[
p\left(\Omega \mid C_1,X_1 \right) \propto p\left( C_1 \mid \Omega,X_1\right).
\]
where we have that for $l\left( \Omega \mid C_1, X_1\right) = \log p\left( C_1 \mid \Omega,X_1\right)$,
\[
l\left( \Omega \mid C_1, X_1\right) = l\left(\hat{\Omega}_{\text{MLE}} \mid C_1, X_1 \right) + \frac{1}{2}\left( \Omega - \hat{\Omega}_{\text{MLE}}\right)^T \nabla^2_{\Omega} l\left(\hat{\Omega}_{\text{MLE}} \right)\left( \Omega - \hat{\Omega}_{\text{MLE}}\right) + o\left( \Vert \Omega - \hat{\Omega}_{\text{MLE}} \Vert^2\right).
\]
Therefore, provided $l\left( \Omega \mid C_1, X_1\right)$ is concave, (or at least the likelihood of other modes goes down to 0 as $n_1 \to \infty$), then
\[
p\left(\Omega \mid C_1,X_1 \right) \approx N\left( \hat{\Omega}_{\text{MLE}}, -\left( \nabla^2_{\Omega} l\left(\hat{\Omega}_{\text{MLE}} \right) \right)^{-1} \right).
\]
I then propose we use
\[
\Omega \mid C_1,X_1 \sim N\left( \hat{\Omega}_{\text{MLE}}, -\left( \nabla^2_{\Omega} l\left(\hat{\Omega}_{\text{MLE}} \right) \right)^{-1} \right)
\]
as the posterior distribution of $\Omega \mid C_1, X_1$ in order to estimate our uncertainty in the estimate $\hat{\Omega}_{\text{MLE}}$. This is very important when computing the variance of our estimator $\hat{B}$ with small $n_1$.

2.) Let $V_{X_1}$ be an orthonormal basis for the kernel of $X_1$, which has $n_1 - d$ dimensions. Let 
\[
\tilde{Y}_1 = Y_1 V_{X_1} = L C_1^T V_{X_1} + \underbrace{\tilde{E}_1}_{MN_{p \times n_1 - d}\left(0, \Sigma, I_{n_1 - d} \right)}
\]
and estimate $L$ on $n_1 - d - K$ degrees of freedom. Call the estimate $\hat{L}$. Note that we can also get an estimate for $\hat{\Sigma}_{p \times p}$, assuming it is diagonal. Note that we could also get an estimate of $\hat{L}$ and $\hat{\Sigma}$ via OLS, including $X_1$ as a covariate.
In the first case
\[
\hat{L} \mid C_1,X_1 \sim MN_{p \times K}\left( L,\Sigma_p,\left( C_1^T V_{X_1} V_{X_1}^T C_1\right)^{-1} \right)
\]
and in the second case (i.e. OLS to find $\hat{L}, \hat{B}$), $\hat{L} = Y_1 P_{X_1^T}^{\perp} C_1 D^{-1}$ and
\[
\hat{L} \mid C_1,X_1 \sim MN_{p \times K}\left( L,\Sigma_p,\left( C_1^T P_{X_1^T}^{\perp} C_1\right)^{-1} \right)
\]
where $D = C_1^T P^{\perp}_{X_1^T}C_1$ and $P^{\perp}_{X_1^T}$ is the projection matrix onto the orhtogonal compliment of the columns of $X_1^T$. Therefore, the two estimators are the SAME. Note that $\hat{L}$ is an unbiased estimator of $L$.

3.) Let $V_{C_1^T}$ be a basis for the kernel of $C_1^T$, which has dimension $n_2 - K$. Denote 
\[
Z_1 = Y_1 V_{C_1^T} \mid C_1, X_1, X_2 \sim MN_{p \times n_1 - K}\left(BX_1 V_{C_1^T}, \Sigma, I_{n_1 - K} \right)
\]

4.) Let $Z_2 = Y_2 - \hat{L} E\left( C_2 \mid C_1 \right)^T = Y_2 - \hat{L}\hat{\Omega}_{\text{MLE}}^T X_2$ be the residuals after removing the effect of the predicted mean cell type. If we condition on $C_1$, $\Omega$ is random. In this procedure, we assume that $\Omega \mid C_1$ and $\hat{L} \mid C_1$ are INDEPENDENT. We then have that $Z_1$ and $Z_2$ are unbiased estimators for $BX_1 V_{C_1^T}$ and $BX_2$:
\[
E\left( Z_1 \mid C_1, X_1, X_2 \right) = BX_1 V_{C_1^T}
\]
\[
E\left(Z_2 \mid C_1, X_1, X_2 \right) = BX_2 + L\hat{\Omega}_{\text{MLE}}^T X_2 - L\hat{\Omega}_{\text{MLE}}^T X_2 = BX_2.
\]
We then must compute the VARIANCE of $\left[ \begin{matrix}
Z_1 & Z_2
\end{matrix} \right]$:

i.) $\text{Var}\left( Z_1 \mid C_1, X_1, X_2 \right)$:
\[
\text{Var}\left( Z_1 \mid C_1, X_1, X_2 \right) = \Sigma_p \otimes I_{n_1 - K}
\]
ii.) $\text{Var}\left( Z_2 \mid C_1, X_1, X_2 \right)$:
\[
\text{Var}\left( \underbrace{\left( Z_2 \right)_i}_{\text{$i^{\text{th}}$ column of $Z_2$}} \mid C_1, X_1, X_2 \right) = \text{Var}\left( \left( Y_2 \right)_i - \hat{L} \hat{\Omega}_{\text{MLE}}^T x_i \mid C_1, X_1, X_2\right) = \text{Var}\left( \left( L\Omega^T-\hat{L}\hat{\Omega}_{\text{MLE}}^T \right) x_i  + L \xi_i + \left( E_2 \right)_i \mid C_1, X_1, X_2 \right)
\]
\[
= \text{Var}\left( \left( L\Omega^T-\hat{L}\hat{\Omega}_{\text{MLE}}^T \right) x_i  + L \xi_i \mid C_1, X_1, X_2 \right) + \Sigma_{p \times p} = E_{\Omega}\text{Var}\left( \left( L\Omega^T-\hat{L}\hat{\Omega}_{\text{MLE}}^T \right) x_i  + L \xi_i \mid C_1, X_1, X_2, \Omega \right) + \cdots
\]
\[
\cdots \text{Var}_{\Omega} E \left[ \left( L\Omega^T-\hat{L}\hat{\Omega}_{\text{MLE}}^T \right) x_i  + L \xi_i \mid C_1, X_1, X_2, \Omega \right] + \Sigma_{p \times p} = \Sigma_{p \times p}  x_i^T \hat{\Omega}_{\text{MLE}} \left( C_1^T P^{\perp}_{X_1^T} C_1 \right)^{-1}\hat{\Omega}_{\text{MLE}}^T x_i + \cdots
\]
\[
\cdots L E\left[ R_i\left( \Omega^T x_i \right) \mid C_1, X_1, X_2 \right]L^T + L S_i L^T = \Sigma_p \left[ 1 + x_i^T \hat{\Omega}_{\text{MLE}} \left( C_1^T P^{\perp}_{X_1^T} C_1 \right)^{-1}\hat{\Omega}_{\text{MLE}}^T x_i \right] + L\left( E\left[ R_i\left( \Omega^T x_i \right) \mid C_1, X_1, X_2 \right] + S_i \right)L^T
\]
And,
\[
\text{Cov}\left( \left( Z_2 \right)_i, \left( Z_2 \right)_j \mid C_1, X_1, X_2 \right) = \text{Cov}\left( L\Omega^T x_i - \hat{L}\hat{\Omega}^T x_i  + L \xi_i + \left( E_2 \right)_i, L\Omega^T x_j - \hat{L}\hat{\Omega}^T x_j + L \xi_j + \left( E_2 \right)_j \mid C_1, X_1, X_2 \right)
\]
\[
 \underbrace{=}_{\text{Same procedure as above}} L S_{i \times j} L^T + \Sigma_p\left[ x_i^T \hat{\Omega}_{\text{MLE}} \left( C_1^T P^{\perp}_{X_1^T} C_1 \right)^{-1}\hat{\Omega}_{\text{MLE}}^T x_j \right]
\]

iii.) $\text{Cov}\left( Z_1, Z_2 \mid C_1, X_1, X_2 \right)$:
\[
\text{Cov}\left( Z_1, Z_2 \mid C_1, X_1, X_2 \right) \underbrace{=}_{\text{Steps Omitted}} - \Sigma_{p \times p} \otimes \left[ V_{C_1^T}^T P^{\perp}_{X_1^T} C_1 \left( C_1^T P^{\perp}_{X_1^T} C_1 \right)^{-1} \hat{\Omega}_{\text{MLE}} X_2 \right]
\]

These calculations make sense, since in the limit as $n_1, n_2 \to \infty$, the covariance between columns disappears and we are left with a variance term that we would use if we knew $L$ and $\Omega$.

5.) Using the above calculations, we can compute the covariance matrix $U_g \in \mathbb{R}^{\left( n_1 -K + n_2 \right) \times \left( n_1 -K + n_2 \right)}$ for a single site $g$ across individuals. If $\beta_g$ is the $g^{\text{th}}$ row of $B \in \mathbb{R}^{p \times d}$, $\ell_g$ the $g^{\text{th}}$ row of $L \mathbb{R}^{p \times K}$ and $Y_i^{(g)}$ the $i^{\text{th}}$ row of $Y_i \in \mathbb{R}^{p \times n_i}$, then
\[
\hat{\beta}_g = \left( \left[ \begin{matrix}
X_1 V_{C_1^T} & X_2
\end{matrix} \right] U_g^{-1} \left[ \begin{matrix}
V_{C_1^T}^T X_1^T\\
X_2^T
\end{matrix} \right]\right)^{-1} \left[ \begin{matrix}
X_1 V_{C_1^T} & X_2
\end{matrix} \right] U_g^{-1} \left[ \begin{matrix}
Y_1^{(g)} V_{C_1^T}^T\\
Y_2^{(g)} - X_2^T \hat{\Omega}_{\text{MLE}} \hat{\ell}_g
\end{matrix} \right]
\]
where
\[
E \hat{\beta}_g = \beta_g
\]
and
\[
\text{Var}\left( \hat{\beta}_g \mid C_1, X_1, X_2 \right) = \left( \left[ \begin{matrix}
X_1 V_{C_1^T} & X_2
\end{matrix} \right] U_g^{-1} \left[ \begin{matrix}
V_{C_1^T}^T X_1^T\\
X_2^T
\end{matrix} \right]\right)^{-1} \stackrel{n_1,n_2 \to \infty}{\longrightarrow} \left( \sigma_g^{-2} X_1 P^{\perp}_{C_1^T} X_1^T + \left( \sigma_g^2 + \ell_g^T R_i \ell_g \right)^{-1} X_2 X_2^T \right)^{-1}.
\]
Note that if we find that the cell proportions are uncorrelated with methylation, then $\ell_g$ would be negligible in comparison to $\sigma_g^2$, which would be the standard regression problem where $B$ was the only coefficient. I think I can also prove that the under regularity conditions, the asymptotic distrubution for $\hat{\beta}_g$ is normally distributed.

We can also simultaneously compute GLS estimates for the multiple sites coefficient vector $\left[ \begin{matrix}
\beta_{g_1}\\
\vdots\\
\beta_{g_m}
\end{matrix} \right]$ if we assume that $\Sigma_{p \times p}$ is diagonal. The analytic form is a bit messy, however.

In the above calculations, there is no need to assume the predicted values for $C_2$ follow a normal distribution. In fact, we ONLY need first and second moment assumptions on the predictive distribution for the variance calculations to be true.

##Investigating Test Statistics: A (biased, lower variance) Quasi-Likelihood Approach
  The above method uses only the first $n_1$ observations to estimate $L$. We can presumably get a lower variance estimate of $B$ if we had a lower variance estimate for $L$. This motivates the quasi-likelihood approach.

Again, we have the same model for methylation:
\[
Y_1 = BX_1 + LC_1^T + E_1
\]
\[
Y_2 = BX_2 + LC_2^T + E_2
\]
We assume that the first and second moments of the predictive distribution for $p\left( C_2 \mid C_1 \right)$ are the same as above.

Estimation will be performed site-by-site. Let the $Y_{1_{g.}} = y_{1_g}$, $Y_{2_{g.}} = y_{2_g}$, $B_{g.} = \beta_g$, $L_{g.} = \ell_g$, $E_{i_{g.}} = \epsilon_{i_g}$, where $E\epsilon_{i_g}= 0$, $E\epsilon_{i_g} \epsilon_{i_g}^T = \sigma_g^2 I_{n_i}$. Define
\[
\mu_{1_g} = \mu_1\left( \beta_g, \ell_g \right) = E\left( y_{1_g} \mid X_1, X_2, C_1 \right) = X_1^T \beta_g + C_1 \ell_g
\]
and
\[
\mu_{2_g} = \mu_2\left( \beta_g, \ell_g \right) = E\left( y_{2_g} \mid X_1, X_2, C_1 \right) = X_2^T \beta_g + X_2^T \hat{\Omega} \ell_g.
\]
Using a method identical to the one in the previous section, we can calculate the variance of $y_{1_g}$ and $y_{2_g}$:
\[
\text{Var}\left( y_{1_g} \mid X_1, X_2, C_1 \right) = \sigma_g^2 I_{n_1}
\]
\[
G(\ell_g) = \text{Var}\left( y_{2_g} \mid X_1, X_2, C_1 \right) = \left( \begin{matrix}
\ell_g^T V_1 \ell_g & \cdots & \ell_g^T S_{1 \times n_2} \ell_g\\
\vdots & \ddots & \vdots\\
\ell_g^T S_{n_2 \times 1} \ell_g & \cdots & \ell_g^T V_{n_2} \ell_g
\end{matrix} \right) + \sigma_g^2 I_{n_2}, \quad V_i = E_{\Omega} R_i + S_i
\]
\[
\text{Cov}\left( y_{1_g}, y_{2_g} \right) = 0.
\]

1.) Estimate $\sigma_g^2$ using the $n_1$ individuals in the training data. The first possible estimate would be the OLS estimate:
\[
\hat{\sigma}_{g,OLS}^2 = \frac{1}{n-t} y_{1_g}^T \left( I_{n_1} - H_1 \right) y_{1_g}
\]
where $H_1$ is the orthogonal projection matrix onto the columns of $\left[ \begin{matrix}
X_1^T & C_1
\end{matrix} \right]$ and $t = d+K =$ number of covariates. It's clear that $\hat{\sigma}_{g,OLS}^2 \stackrel{P}{\to} \sigma_g^2$ under very minor assumptions on $X_1$ (i.e. it has full rank $\forall$ $n_1 > d$).

If $n_1$ is small, we can take a Bayesian approach and pool our data to estimate each $\sigma_g^2$ jointly. Let $Q_g \in \mathbb{R}^{n_1 \times \left( n_1-t \right)}$ be an orthonormal basis for $\text{ker}\left( \left[ \begin{matrix}
X_1^T & C_1
\end{matrix} \right]^T \right)$. If we then assume that $\epsilon_{1_g} \mid \sigma_g^2 \sim N\left( 0, \sigma_g^2 I_{n_1} \right)$, we can use the following hierarchical model to get a lower variance estimate of $\sigma_g^2$:
\[
Q_g^T y_{1_g} = \tilde{y}_{1_g} \mid \sigma_g^2 \sim N\left( 0, \sigma_g^2 I_{n_1 - t} \right)
\]
\[
\sigma_g^2 \stackrel{\text{iid}}{\sim} IG\left( \alpha, \gamma \right)
\]
and set
\[
\hat{\sigma}_{g,B}^2 = E\left[ \sigma_g^2 \mid \tilde{y}_{1_1}, \ldots, \tilde{y}_{1_p} \right], \quad \sigma_g^2 \mid \tilde{y}_g \sim IG\left( \hat{\alpha} + \frac{n_1 - t}{2}, \hat{\gamma} + \frac{\tilde{y}_g^T \tilde{y}_g}{2} \right)
\]
where $\hat{\alpha}$ and $\hat{\gamma}$ are estimated by ML. To prove $\hat{\sigma}_{g,B}^2 \stackrel{P}{\to} \sigma_g^2$, we need only show that we can control the size of $\hat{\alpha}$ and $\hat{\gamma}$, that is we need to prove
\[
\limsup \hat{\alpha}, \text{ } \limsup \hat{\gamma} = o_P\left( n_1 \right)
\]

2.) Estimate $\ell_g$ and $\beta_g$ using quasi-likelihood (see McCullagh, Nelder). Suppose first that $\sigma_g^2$ was known. Let $\tilde{\beta}_g = \left( \begin{matrix}
\beta_g\\
\ell_g
\end{matrix} \right)$. The score equations would be
\[
U_1 = \frac{1}{\sigma_g^2}\left[ \begin{matrix}
X_1\\
C_1^T
\end{matrix} \right] \left( y_{1_g} - \mu_{1_g} \right)
\]
\[
U_2 = \left[ \begin{matrix}
X_2\\
\hat{\Omega}^T X_2
\end{matrix} \right]G\left( \ell_g \right)^{-1}\left( y_{2_g} - \mu_{2_g} \right)
\]
\[
U = U_1 + U_2
\]
with
\[
T = -E\frac{dU}{d\tilde{\beta}} = \text{Var} U = \left[ \begin{matrix}
\frac{1}{\sigma_g^2}X_1 X_1^T + X_2 G\left( \ell \right)^{-1} X_2^T & \frac{1}{\sigma_g^2}X_1 C_1 + X_2 G\left( \ell \right)^{-1} X_2^T \hat{\Omega}\\
\frac{1}{\sigma_g^2}C_1^T X_1^T + \hat{\Omega}^T X_2 G\left( \ell \right)^{-1} X_2^T & \frac{1}{\sigma_g^2} C_1^T C_1 + \hat{\Omega}^T X_2 G\left( \ell \right)^{-1} X_2^T \hat{\Omega}
\end{matrix} \right].
\]
and Newton updates
\[
\hat{\tilde{\beta}}_{k+1} = \hat{\tilde{\beta}}_k + T_k^{-1} U_k.
\]
Using standard asymptotic quasi-likelihood theory (see McCullagh, Nelder and QuasiLikelihood_Intro.pdf), 
\[
T^{1/2} \left( \left( \begin{matrix}
\hat{\beta}_g\\
\hat{\ell}_g
\end{matrix} \right) - \left( \begin{matrix}
\beta_g\\
\ell_g
\end{matrix} \right) \right) \stackrel{\mathcal{D}}{\to} N\left( 0, I_t \right).
\]

If $\sigma_g^2$ is NOT known, then we can use one of our estimators as a plugin estimator for $\sigma_g^2$. To sketch a proof of why this is appropriate, let $\hat{\sigma}_g^2 = \hat{\sigma}_g^2 \left( \tilde{y}_{1_g} \right)$ be an estimator of $\sigma_g^2$ s.t. $\hat{\sigma}_g^2 \stackrel{P}{\to} \sigma_g^2$ and define
\[
\hat{G} = \hat{G}\left( \ell_g \right) = \left( \begin{matrix}
\ell_g^T V_1 \ell_g & \cdots & \ell_g^T S_{1 \times n_2} \ell_g\\
\vdots & \ddots & \vdots\\
\ell_g^T S_{n_2 \times 1} \ell_g & \cdots & \ell_g^T V_{n_2} \ell_g
\end{matrix} \right) + \hat{\sigma}_g^2 I_{n_2}
\]
\[
\hat{U}\left( \tilde{\beta} \right) = \left[ \begin{matrix}
X_1 & X_2\\
C_1^T & \hat{\Omega}^T X_2
\end{matrix} \right]\left( \begin{matrix}
\frac{y_{1_g} - \mu_{1_g}}{\hat{\sigma}_g^2}\\
\hat{G}^{-1} \left( y_{2_g} - \mu_{2_g} \right)
\end{matrix} \right).
\]
and Newton updates
\[
\hat{\hat{\tilde{\beta}}}_{k+1} = \hat{\hat{\tilde{\beta}}}_k + \hat{T}_k^{-1} \hat{U}_k \quad (*).
\]
If we assume that $y_{1_g}$ and $y_{2_g}$ are INDEPENDENT, then $\hat{\sigma}_g^2 \perp y_{2_g}$. Further, if $y_{1_g} \mid X_1, C_1 \sim N\left( \mu_{1_g}, \sigma_g^2 I_{n_1} \right)$, then $\hat{\sigma}_g^2 \perp \left( \begin{matrix}
X_1\\
C_1^T
\end{matrix} \right) y_{1_g}$. We therefore have


a.)
\[
E\left( \hat{U} \mid \hat{\sigma}_g^2 \right) = 0 \Rightarrow E \hat{U} = EE\left( \hat{U} \mid \hat{\sigma}_g^2 \right) = 0.
\]
b.)
\[
\text{Var}\hat{U} = E_{\hat{\sigma}_g^2} \text{Var}\left( \hat{U} \mid \hat{\sigma}_g^2 \right) + \text{Var}_{\hat{\sigma}_g^2}\underbrace{E\left( \hat{U} \mid \hat{\sigma}_g^2 \right)}_{=0} = E_{\hat{\sigma}_g^2} \left( \left[ \begin{matrix}
X_1 & X_2\\
C_1^T & \hat{\Omega}^T X_2
\end{matrix} \right] \left[ \begin{matrix}
\frac{\sigma_g^2}{\hat{\sigma}_g^2 \hat{\sigma_g}^2} I_{n_1} & 0\\
0 & \hat{G}^{-1} G \hat{G}^{-1}
\end{matrix} \right] \left[ \begin{matrix}
X_1^T & C_1\\
X_2^T & X_2^T\hat{\Omega}
\end{matrix} \right] \right)
\]
c.)
\[
\hat{T} = -E \left( \frac{d\hat{U}}{d\tilde{\beta}} \mid \hat{\sigma}_g^2 \right) = \left[ \begin{matrix}
X_1 & X_2\\
C_1^T & \hat{\Omega}^T X_2
\end{matrix} \right] \left[ \begin{matrix}
\frac{1}{\hat{\sigma}_g^2} I_{n_1} & 0\\
0 & \hat{G}^{-1}
\end{matrix} \right] \left[ \begin{matrix}
X_1^T & C_1\\
X_2^T & X_2^T\hat{\Omega}
\end{matrix} \right]= \left[ \begin{matrix}
\frac{1}{\hat{\sigma}_g^2}X_1 X_1^T + X_2 \hat{G}^{-1} X_2^T & \frac{1}{\hat{\sigma}_g^2}X_1 C_1 + X_2 \hat{G}^{-1} X_2^T \hat{\Omega}\\
\frac{1}{\hat{\sigma}_g^2}C_1^T X_1^T + \hat{\Omega}^T X_2 \hat{G}^{-1} X_2^T & \frac{1}{\hat{\sigma}_g^2} C_1^T C_1 + \hat{\Omega}^T X_2 \hat{G}^{-1} X_2^T \hat{\Omega}
\end{matrix} \right]
\]


We also have the identity
\[
\hat{G}^{-1} \underbrace{=}_{\rho_{n_2}\left( G \right) \geq \sigma_g^2} G^{-1}\left( I_{n_2} - \left( \hat{\sigma}_g^2 - \sigma_g^2 \right)G^{-1} + \left( \hat{\sigma}_g^2 - \sigma_g^2 \right)^2 G^{-2} + o_P\left( \left( \frac{\hat{\sigma}_g^2 - \sigma_g^2}{\sigma_g^2} \right)^2 \right) \right)
\]
\[
\Rightarrow X_2\hat{G}^{-1} X_2^T \stackrel{P}{\sim} X_2 G^{-1} X_2^T \Rightarrow \hat{T}T^{-1} \stackrel{P}{\to} I_t.
\]
Further,
\[
\text{Var}\left( \hat{T}^{-1}\hat{U} \right) = E_{\hat{\sigma}_g^2}\left( \hat{T}^{-1} \left[ \begin{matrix}
X_1 & X_2\\
C_1^T & \hat{\Omega}^T X_2
\end{matrix} \right] \left[ \begin{matrix}
\frac{\sigma_g^2}{\hat{\sigma}_g^2 \hat{\sigma_g}^2} I_{n_1} & 0\\
0 & \hat{G}^{-1} G \hat{G}^{-1}
\end{matrix} \right] \left[ \begin{matrix}
X_1^T & C_1\\
X_2^T & X_2^T\hat{\Omega}
\end{matrix} \right] \hat{T}^{-1}  \right) \underbrace{\to}_{\text{technical, but doable argument needed}} 0
\]
meaning the estimator with updates given by (*) is consistent. Lastly, we wish to show that $\hat{\hat{\tilde{\beta}}}_g - \hat{\tilde{\beta}}_g = o_P\left( \norm{T^{-1/2}} \right)$ so that $T^{1/2}\left( \hat{\hat{\tilde{\beta}}}_g - \hat{\tilde{\beta}}_g \right) = o_P\left( 1 \right)$. To prove this, we note that
\[
\hat{T}^{-1}\hat{U} = \left(\frac{1}{n_1 + n_2}\hat{T}\right)^{-1} \frac{1}{n_1 + n_2} \hat{U} = \hat{\bar{T}}^{-1} \hat{\bar{U}} = \hat{\bar{T}}^{-1} \left( \bar{U} + \hat{\bar{R}} \right)
\]
where
\[
\hat{\bar{R}} = \bar{U} - \hat{\bar{U}} \stackrel{P}{\to} 0.
\]
Therefore if we start at $\tilde{\beta}_g$ and for $n_1, n_2$ large enough,
\[
\hat{\hat{\tilde{\beta}}}_g - \hat{\tilde{\beta}}_g \approx \left(\hat{\bar{T}}^{-1} - \bar{T}^{-1} \right)\bar{U} + \hat{\bar{T}}^{-1}\hat{\bar{R}}
\]
\[
\Rightarrow T^{1/2} \left( \hat{\hat{\tilde{\beta}}}_g - \hat{\tilde{\beta}}_g \right) = o_P\left( 1 \right) 
\]
Therefore,
\[
\hat{T}^{1/2}\left( \hat{\hat{\tilde{\beta}}}_g - \tilde{\beta}_g \right) \stackrel{\mathcal{D}}{\to} N\left( 0,I_t \right).
\]

## Simulate Data

Fix $\Omega$, $X_1$, $X_2$, $C_1$, $B$, $L$, $\Sigma$

```{r Initialize}
Omega.sim = Omega[,1:3];  
```

## Session information

```{r info}
sessionInfo()
```
