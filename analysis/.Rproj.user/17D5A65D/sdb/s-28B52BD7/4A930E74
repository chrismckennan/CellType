{
    "contents" : "---\ntitle: \"Investigate cell type and how it relates to covariates\"\nauthor: \"Chris McKennan\"\ndate: 2016-02-05\n---\n\n**Last updated:** `r Sys.Date()`\n\n**Code version:** `r system(\"git log -1 --format='%H'\", intern = TRUE)`\n\n```{r chunk-options, include=T}\nsource(\"chunk-options.R\")\nsource(\"../R/OptimizeLogLike.R\")\nsource(\"../R/OptimizeLogLike_Ksigma.R\")\n```\n\nThe purpose of this file is to investigate how cell type is related to the covariates. That is, do we have any hope in predicting cell type given an individuals covariate information. In order to apply our supervised method, we need the variance in cell type among unrelated individuals to be small, given the covariate information.\n\n## Load required functions\n\n```{r}\nlibrary(minfi)\nlibrary('IlluminaHumanMethylation450kmanifest')\nlibrary('IlluminaHumanMethylation450kanno.ilmn12.hg19')\nlibrary('FlowSorted.Blood.450k')      ##Methylation data on 6 males, 10 cell types; RGset object\nlibrary('RefFreeEWAS')\nlibrary('nlme')\nlibrary('corpcor')\nlibrary('sva')\nlibrary('knitr')\nlibrary('printr')\n```\n\n\n## Get data into R\n\nGet the data into R and initialize global variables.\n\n```{r}\npath.LPS.kids <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/HTkids_cellprop/HTkids_LPS_covar_ImpAct_10115.txt'\npath.methylation <- '/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/Hutterite_Methylation_data/Meth_covar_allsamp_impcells_10115.txt'\ndata.cov <- data.frame(read.table(path.methylation, sep=\"\\t\", quote=\"\\\"\", dec=\".\", header=T, check.names=F))\ndata.LPS <- data.frame(read.table(path.LPS.kids, sep=\"\\t\", quote=\"\\\"\", dec=\".\", header=T, check.names=F))\ncell.names <- colnames(data.LPS)[(ncol(data.LPS) - 5):ncol(data.LPS)]\nn.cells <- length(cell.names)\nn.ind <- nrow(data.LPS)\nind.ids.cell <- data.LPS$Rowid    #IDs of individuals with cell type data\nind.ids.covar <- data.cov$Rowid   #IDs of individuals in complete covariate file\n```\n\n## How well do covariates predict cell type\nCell types are on regular 0-1 scale\n\n```{r}\nage.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)\nmodel.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)\nbeta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))\nH.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))\nY.mat <- as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])\nY.mat.hat <- H.cov %*% Y.mat\nbeta.mat <- beta.op.cov %*% Y.mat\nvar.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))\n\ni = 3\nplot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))\nlines(seq(0,100, 100), seq(0, 100, 100))\n\nvar.beta.hat <- diag(1/diag(solve(t(model.mat.cov) %*% model.mat.cov)))\nt.scores <- sqrt(var.beta.hat) %*% beta.mat %*% diag(1/sqrt(var.vec))\n\n```\n\n\n##Multivariate Gaussian with Dispersed Multinomial Covariance Structure\n\nThe model I fit below assumes that for $p_i = \\Omega^T x_i$,\n\\[\nc_i \\sim N\\left(p_i, \\Sigma_i\\right), \\quad \\Sigma_i = \\text{diag}\\left(\\sigma_1, \\ldots, \\sigma_K\\right) \\left(\\text{diag}\\left( p_i \\right) - p_i p_i^T\\right) \\text{diag}\\left(\\sigma_1, \\ldots, \\sigma_K\\right)\n\\]\n\nFirst, I will get an estimate of $\\sigma_k^2$ using the above simple univariate regressions. This will be used as a starting point in the optimization procedure. If $v_k$ is the variance in the OLS regression performed above, then $\\sigma_k^2 = \\frac{v_k}{p(1-p)}$, where $p$ is the cell type proportion. We need this to be small for our optimization/downstream method to perform well.\n```{r Estimate sigma2}\nvar.OLS <- var.vec/1e4    #Michelle's proportions were given in %\nave.hat <- apply(Y.mat.hat, 2, mean)/100   #Average predicted proportion = average measured cell type proportion across individuals\nsigma2.vec <- var.OLS/ave.hat/(1-ave.hat)\nsigma2.0 <- median(sigma2.vec)   #Starting value of sigma^2 for optimization procedure. It corresponds to about 38 observations (a little small, although enough to make the ).\n```\n\nSince our naive estimated $\\sigma^2$ is relatively large compared to the measured B cells and monocytes are relatively small (~1% and 0.5%, respectively), we do not have the power to accurately estimate their means. Thereore, we should consider combining them into the 'other' category. Eosinophil's are right on the border of what we have the power to estimate (note that these have been linked to the devlopment of asthma, so it may be crucial that we include them).\n\nIn this next part, I will get starting values for the mean. I will pool B cells and Monocytes into the 'Other' category.\n```{r Get Initial Estiamats for the mean}\npool.cells <- c('Bcell.act')\nC <- Y.mat[,! colnames(Y.mat) %in% pool.cells]\nC[,which(colnames(C)=='Other.act')] = C[,which(colnames(C)=='Other.act')] + apply(cbind(Y.mat[,colnames(Y.mat) %in% pool.cells]), 1, sum)\nC <- C[,-which(colnames(C)=='Other.act')]/100   #C is cell type matrix to be used in optimization\nOmega.0 <- beta.mat[,! colnames(Y.mat) %in% pool.cells]/100\nOmega.0 <- Omega.0[,-which(colnames(Omega.0)=='Other.act')]\n\nage.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)\nX <- as.matrix(model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS))\n```\n\nOptimize the likelihood using a Newton line-search with $K$ dispersion parameters\n```{r Optimize Likelihood}\nn <- nrow(X)\nd <- ncol(X)\nK <- ncol(C)\ngrad.tol <- 1e-10   #Gradient tolerance\nML <- MaxLike.NewtonLS_Ksigma(X, C, Omega.0, rep(sigma2.0^0.5,K), grad.tol)\nOmega <- ML$Omega    #A covariate x cell type matrix\nFI <- ML$I     #Fisher information matrix at optimum\ncov.theta <- solve(FI)\nHess <- ML$Hessian\nsigma <- ML$sigma    #sigma (i.e. #multinomial observations ~ 1/sigma^2)\nmean.cell <- X %*% Omega\nstand.Omega <- Omega/matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)\nsd.Omega <- matrix(sqrt(diag(cov.theta)[1:(d*K)]), nrow=d, ncol=K)\ncolnames(sd.Omega) <- colnames(Omega)\nrownames(sd.Omega) <- rownames(Omega)\n\ntmp.sigma <- matrix(sigma, nrow=1, ncol=K)\ncolnames(tmp.sigma) <- paste0('sigma_', as.character((1:K)))\ntmp.pred.sigma <- rbind(Omega[1,])/(sqrt(rbind(Omega[1,]*(1-Omega[1,]))) * tmp.sigma)\nknitr::kable(tmp.sigma, digits=4, caption=\"Estimates for sigma's\", longtable=F)\nknitr::kable(tmp.pred.sigma, digits=4, caption=\"Standardized Intercepts (i.e. how accurate is our prediction)\", longtable=F)\nknitr::kable(head(Omega.0), digits = 4, caption = \"Omega using Univariate OLS\", longtable=F)\nknitr::kable(head(Omega), digits = 4, caption = \"Omega using Multivariate Maximum Likelihood\", longtable=F)\nt.scores.table <- t.scores[,! colnames(Y.mat) %in% c('Other.act', pool.cells)]\ncolnames(t.scores.table) <- colnames(Omega)\nrownames(t.scores.table) <- rownames(Omega)\nknitr::kable(head(sd.Omega), digits = 4, caption = \"SD of Estimated Effects using Fisher Information of ML Estimate\", longtable=F)\nknitr::kable(head(stand.Omega), digits = 4, caption = \"Standardized Effects using Fisher Information of ML Estimate\", longtable=F)\n```\n\n##Are our data overdispersed?\nWhile the estimate $\\hat{\\sigma_k}^2_{\\text{ML}}$ is the maximum likelihood estimate, I want to make sure the variance of the residuals is not is not larger than we would expect from our ML model (this should not be the case, as our estimated parameters make the data the most 'normal'). Let $\\hat{\\mu}_i = \\hat{\\Omega}_{\\text{ML}}^T x_i$, $S_i^T S_i = \\hat{\\Sigma}_i = \\hat{\\sigma}^2_{\\text{ML}}\\left[\\text{diag}\\left( \\hat{\\mu}_i \\right) - \\hat{\\mu}_i \\hat{\\mu}_i^T \\right]$ and the residual for individual $i$ be $\\hat{r}_i = c_i - \\hat{\\mu}_i$. Then $S_i^{-T} \\hat{r}_i$ is approximately $N(0,1)$. Using standard multivariate regression theory, we can estimate the dispersion as\n\\[\n\\text{disp} = \\left(\\frac{\\text{det}\\left( \\frac{1}{n-K}\\hat{R}^T \\hat{R}\\right)}{E\\det(W_K)}\\right)^{1/K}\n\\]\nwhere $W_K \\sim \\mathcal{W}_K \\left( I_K, n-d\\right)$. We (obviously) find that there is no dispersion with our estimated parameters.\n\n```{r Estimate Overdispersion}\nResid <- C - mean.cell\nStand.Resid <- C - mean.cell\nfor (i in 1:n) {\n  p.i <- mean.cell[i,]\n  Sigma.i <- diag(sigma) %*% (diag(p.i) - cbind(p.i) %*% rbind(p.i)) %*% diag(sigma)\n  R.i <- chol(Sigma.i)   #R.i'R.i = Sigma.i\n  Stand.Resid[i,] = as.vector( solve(t(R.i), Resid[i,]) )   #Standardized residuals should be iid N(0,1)\n}\n\n##Look at distribution of det(Wishart) to get an idea how large Stand.Resid are##\n#Note E|W| = n^{-p} * n * ... * (n - p + 1), where W ~ Wishart_p(I_p, n), since n^p|W| ~ \\chi^2_n * ... \\chi^2_{n-p+1}, where the \\chi^2 are independent\nprecision.wish <- n\nmean.detW <- exp( sum(log( (precision.wish:(precision.wish - K + 1)) )) ) * precision.wish^(-K)\ndisp <- (det(t(Stand.Resid) %*% Stand.Resid/precision.wish) / mean.detW)^(1/K)\n#sigma2.final <- sigma2 * disp\n#Note that the minimum and maximum eigen values are well within the range of a random Wishart matrix with dimension K and n observations. This does not validate the method, though. It just means we are not too wrong.\n```\n\nUsing the Fisher information at our estimated parameters as an estimate of their covariance, we come to the same conclusion about their significance using the univariate regression (the Fisher information includes the estimate for $\\sigma_k^2$). I think this is a feasible method to estimate cell-type, provided we have enough individuals. The only problem is that for Eos.act, $\\hat{\\sigma_k}_{\\text{ML}} = 0.14$, meaning our predictions will protrude through the simplex 93% of the time...\n\nWe might be able to use this as a rough estimate for the variance of our predictions. That way, we can determine how much individuals with unobserved cell types will help in our estimation.\n\n##Probit-transform of the data\nThe only statistically significant predictors in the univariate regression are the intercept in all regressions (obviously) and the effect on age in 'other' cells. Maybe we can completely ignore the fact that these depend on covariates? They do not appear to depend on them...\n\ncell types are probit-transformed:\n\n```{r}\nage.cutoff <- 11    ##People ages <= age.cutoff are in one group, > age.cutoff are in another (pre-puberty and puberty)\nmodel.mat.cov <- model.matrix(~sex + asthma + as.numeric(Age > age.cutoff), data=data.LPS)\nbeta.op.cov <- solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))\nH.cov <- model.mat.cov %*% solve(t(model.mat.cov) %*% model.mat.cov, t(model.mat.cov))\nY.mat <- qnorm(as.matrix(data.LPS[,(ncol(data.LPS) - 5):ncol(data.LPS)])/100)\nY.mat.hat <- H.cov %*% Y.mat\nbeta.mat <- beta.op.cov %*% Y.mat\nvar.vec <- diag( t(Y.mat) %*% Y.mat - t(Y.mat) %*% Y.mat.hat)/(n.ind - ncol(model.mat.cov))\n\ni = 3\nplot(Y.mat.hat[,i], Y.mat[,i], xlim=range(Y.mat.hat[,i], Y.mat[,i]), ylim=range(Y.mat.hat[,i], Y.mat[,i]))\nlines(seq(0,100, 100), seq(0, 100, 100))\n```\n\nThe conclusion is that regardless of scale (probit-transformed or regular scale), the current covariates (age, sex, asthma status) are a poor predictor of cell type. It also looks like we can get away with binning ages.\n\n## Is logit(beta value) an appropriate scale to do regression against cell type (hopefully yes, so we can assume a normal response)\n\nLoad Michele's data\n\n```{r}\n#Load SWAN-normalized data\n#Variables are Mset.swan.norm and rgset\nload(file=\"/Users/Chris/Desktop/Uchicago/Nicolae/DNAMethylation/MichellesData/MyWork/SavedObjects/Rfile.RData\")\nmeth.norm <- getMeth(Mset.swan.norm)\nunmeth.norm <- getUnmeth(Mset.swan.norm)\nbeta.values <- meth.norm/(meth.norm+unmeth.norm+100)  #These are ordered by row in data.cov\n```\n\nIdentify columns of the data that correspond to the data that I have in data.LPS (i.e. cell type data) and perform a regression\n\n```{r}\ncol.use <- match(ind.ids.cell, ind.ids.covar)\nbeta.cell <- beta.values[,col.use]\nM.cell <- log(beta.cell/(1-beta.cell))\nmodel.mat <- model.matrix(~Tcell.act + Bcell.act + Eos.act + Neut.act + Mono.act, data=data.LPS)\nbeta.op <- solve(t(model.mat) %*% model.mat, t(model.mat))\nH <- model.mat %*% beta.op\nH.perp <- diag(1,ncol(H)) - H\n\ncoef.beta <- beta.op %*% t(beta.cell)\ncoef.M <- beta.op %*% t(M.cell)\n\nSSreg.beta <- apply(beta.cell %*% H, 1, var)\nSSreg.M <- apply(M.cell %*% H, 1, var)\nSSY.beta <- apply(beta.cell, 1, var)\nSSY.M <- apply(M.cell, 1, var)\n\nR2.beta <- SSreg.beta/SSY.beta\nR2.M <- SSreg.M/SSY.M\n\nplot(R2.beta, R2.M, pch=\".\", xlim=range(R2.M, R2.beta), ylim=range(R2.M, R2.beta), main='Comparison of Variance Explained by Cell type', xlab='R^2 when Y = beta value', ylab='R^2 when Y = M value')\nlines(seq(0,1,length=100), seq(0,1,length=100), col=\"red\")\n```\n\nFrom this, it looks like we can use M-values when dealing with confounding due to cell type. It also appears cell type is NOT very dependent on these coefficients, but they may be dependent in other data sets.\n\n## How well does Houseman's method do in predicting cell type?\n\nI now want to look at how well Houseman's method perform's in predicting cell type in comparison to using training data. If it does well, there is no need to use training data. We can just use Houseman's trainin data.\n\nHow Tcell prediction differs between Houseman's 2012 method and a simple regression using Hutterite cell proportions (I added Houseman's CD4 and CD8 estimates to get his 'Tcell' proportion)\n```{r}\nHouse.cell <- data.LPS[(ncol(data.LPS)-12):(ncol(data.LPS)-7)]\nTcell.house <- House.cell[,1] + House.cell[,2]\nTcell.act <- data.LPS$Tcell.act\nrisk.house <- sum((Tcell.house*100 - Tcell.act)^2)/length(Tcell.act)\n\nTcell.hat <- H.cov %*% cbind(Tcell.act)\nLOO.cv <- sqrt(mean( (Tcell.act - Tcell.hat)^2/(1 - diag(H.cov))^2 ))\nprint(paste0('Average L2 Loss for Houseman Tcells = ', as.character(signif(risk.house, digits=4))))\nprint(paste0('Average LOO CV L2 Loss for Regression Tcells = ', as.character(signif(LOO.cv, digits=4))))\n```\n\nNow with Bcells:\n```{r}\nBcell.house <- House.cell$Bcell\nBcell.act <- data.LPS$Bcell.act\nB.risk.house <- sum((Bcell.house*100 - Bcell.act)^2)/length(Bcell.act)\n\nBcell.hat <- H.cov %*% cbind(Bcell.act)\nB.LOO.cv <- sqrt(mean( (Bcell.act - Bcell.hat)^2/(1 - diag(H.cov))^2 ))\nprint(paste0('Average L2 Loss for Houseman Bcells = ', as.character(signif(B.risk.house, digits=4))))\nprint(paste0('Average LOO CV L2 Loss for Regression Bcells = ', as.character(signif(B.LOO.cv, digits=4))))\n```\n\n## Conclusion\n\n1. Houseman's training data does a poor job in predicting cell type in comparison to a simple linear regression with cell proportion training data.\n\n2. At least in the Hutterite data set, cell proportion looks to be independent of asthma status, age (all individuals were 7-14 years old) and gender. They may be more dependent on covariates for different data sets, however.\n\n3. We can estimate cell proportions using a Gaussian model with multinomial likelihood. With enough individuals we can get a good estimate of $\\hat{\\Omega}$. However, we estimate $\\sigma \\approx 0.18$, meaning our prediction may portrude through the simplex (see standardized effects table above).\n\n4. We still need to understand how much additional information we can get from individuals with unmeasured cell types to estimate main effects of interest.\n\n\n##How Much Additional Information do we get form Individuals with Unobserved Cell Types?\nWe suppose we have two groups of individuals, one with observed cell types and one without cell types. We model both these groups as\n\\[\nY_1 = B_{p \\times d}X_1 + L_{p \\times k}C_1^T + E_1, \\quad Y_1 \\in \\mathbb{R}^{p \\times n_1}\n\\]\n\\[\nY_2 = BX_2 + L\\Omega_{K \\times d}^T X_2 + L\\Xi_{K \\times n_2} + E_2, \\quad Y_2 \\in \\mathbb{R}^{p \\times n_2}.\n\\]\nWe assume that the columns of $E_i$ are indpendent. For now, $\\Xi_{K \\times n_2} = \\left( \\xi_1 \\cdots \\xi_{n_2} \\right)$, where $\\xi_i \\sim N_K\\left( 0,\\Sigma_i \\right)$, $p_i = \\Omega^T_{K \\times d}x_i$ and\n\\[\n\\Sigma_i = \\text{diag}\\left(\\sigma_1, \\ldots, \\sigma_K\\right) \\left(\\text{diag}\\left( p_i \\right) - p_i p_i^T\\right) \\text{diag}\\left(\\sigma_1, \\ldots, \\sigma_K\\right).\n\\]\nWe first suppose that $L_{p \\times K}$ with rows $\\ell_g$, $g = 1, \\ldots, p$ and $\\Omega_{d \\times K}$ are known. We are interested in determining how must additional information $Y_2$ gives us, which can be measured by how much the variance of $\\hat{B}$ shrinks with the additional observations (*).\n\nAs an aside, if $n_1 = 0$, then we are back to the unsupervised scenario. I was originally modeling $\\Xi$ as a $MN\\left(0, I_K, I_{n_2}\\right)$. However, for strong covariates in $X_2$, the vectors $\\xi_i$ have non-identical covariance that depends on $X_2$, which invalidates the methodology I had proposed. If, however, there are only a few covariates (like disease vs. non-disease), then we can apply our favorite factor analysis method to get an estimate for $L$ by splitting individuals by diseased and non-diseased.\n\nTo study (*), we look at a very simple example. We consider a scenario when the only covariate of interest is disease status (i.e. yes or no covariates). For site $g$, let $\\beta_g$ be the $g^{\\text{th}}$ row of $B$. Our model is then\n\\[\nY_{1_g} = X_1 \\beta_g + C_1 \\ell_g + \\epsilon_{1_g}\n\\]\n\\[\nY_{2_g} = X_2 \\beta_g + X_2 \\Omega_{d \\times K} \\ell_g + \\Xi^T_{n_2 \\times K} \\ell_g + \\epsilon_{2_g}\n\\]\nwhere $\\epsilon_{i_g} \\sim N\\left(0, \\sigma_g^2 I_{n_i}\\right)$ and the columns of $\\Xi_{K \\times n_2}$, $\\xi_i$, are distributed as above (with covariance $\\Sigma_i$ that depends on $X_2$). If $\\Omega$ and $L$ are known, then the variance for individual $i$ in group 1 is $\\sigma_g^2$ and if $i$ is in group 2 it is $\\sigma_g^2 + \\ell_g^T \\Sigma_i \\ell_g$. Note that individuals remain independent when $\\Omega$ and $L$ are known. For simplicity, we assume there are an equal number of diseased and non-diseased individuals in each group. Let $S_j = \\text{var}\\left(Y_j\\right)$, $j = 1,2$. Then\n\\[\n\\text{var}\\left(\\hat{\\beta}_g\\right) = \\left( X_1^T S_1^{-1}X_1 + X_2^T S_2^{-1}X_2 \\right)^{-1} = \\text{diag}\\left( \\frac{n_1}{2}\\sigma_g^{-2} + \\sum\\limits_{i=1}^{n_2/2}\\left( \\sigma_g^2 + \\ell_g^T \\Sigma_i \\ell_g\\right)^{-1}, \\frac{n_1}{2}\\sigma_g^{-2} + \\sum\\limits_{i=n_2/2}^{n_2}\\left( \\sigma_g^2 + \\ell_g^T \\Sigma_i \\ell_g\\right)^{-1}\\right)^{-1}\n\\]\n\\[\n= \\text{diag}\\left( \\frac{n_1}{2}\\sigma_g^{-2} + \\frac{n_2}{2}\\ell_g^T \\Sigma_{\\text{Asthma}}\\ell_g, \\frac{n_1}{2}\\sigma_g^{-2} + \\frac{n_2}{2}\\ell_g^T \\Sigma_{\\text{No Asthma}}\\ell_g \\right)^{-1}\n\\]\nWithout the individuals from $Y_2$,\n\\[\n\\text{var}\\left(\\hat{\\beta}_g\\right) = \\text{diag}\\left( \\frac{2}{n_1}\\sigma_g^2, \\frac{2}{n_1}\\sigma_g^2 \\right).\n\\]\nTherefore, the reduction of the variance of treatment difference is exactly\n\\[\n\\frac{1}{2}\\left( \\frac{1}{1 + \\frac{n_2}{n_1} \\frac{\\sigma_g^2}{\\alpha_{\\text{Asthma}}}} + \\frac{1}{1 + \\frac{n_2}{n_1} \\frac{\\sigma_g^2}{\\alpha_{\\text{No Asthma}}}} \\right)\n\\]\nwhere $\\alpha_{k} = \\sigma_g^2 + \\ell_g^T \\Sigma_k \\ell_g$.\n\nBelow, I will estimate $L$ and $\\sigma_g^2$ from Michelle's data and use those estimates to determine the reduction of variance.\n\n```{r Determine L and Variance}\nm <- nrow(M.cell)\nX.L <- model.matrix(~Tcell.act + Eos.act + Neut.act + Mono.act + asthma + sex + as.numeric(Age > age.cutoff), data=data.LPS)\nbeta.op.L <- solve(t(X.L) %*% X.L, t(X.L))\nH.L <- X.L %*% solve(t(X.L) %*% X.L, t(X.L))\nperp.L <- diag(n) - H.L\nbeta.L <- M.cell %*% t(beta.op.L)\nL <- beta.L[,2:5]    #In the orderof Tcell, Eosinophil, Neutrophil, Monocyte\nLO <- L %*% t(Omega)     #This is the potion of the effect due to cell type, i.e. Y = (B + LO)X + E. We want to determine how large B is in relation ot LO. If B is large, then cell type does NOT contribute to confounding\nB <- beta.L[,6:8]\nvar.g.vec <- rep(0, m)     #An estimate of sigma^2_g for each site g = 1,...,p\nfor (g in 1:m) {\n  tmp.site <- cbind(M.cell[g,])\n  var.g.vec[g] <- as.numeric(t(tmp.site) %*% perp.L %*% tmp.site / (n - ncol(X.L)))\n}\nx.asthma <- cbind(c(1,0,1,0))\nx.nasthma <- cbind(c(1,0,0,0))\np.asthma <- t(Omega) %*% x.asthma\np.nasthma <- t(Omega) %*% x.nasthma\nSigma.asthma <- diag(sigma) %*% (diag(as.vector(p.asthma)) - p.asthma %*% t(p.asthma)) %*% diag(sigma)\nSigma.nasthma <- diag(sigma) %*% (diag(as.vector(p.nasthma)) - p.nasthma %*% t(p.nasthma)) %*% diag(sigma)\n\nvar.cell.asthma <- rep(0,m)    #l' Sigma_asthma l\nvar.cell.nasthma <- rep(0,m)    #l' Sigma_noasthma l\nfor (g in 1:m) {\n  l.g <- cbind(L[g,])\n  var.cell.asthma[g] <- t(l.g) %*% Sigma.asthma %*% l.g\n  var.cell.nasthma[g] <- t(l.g) %*% Sigma.nasthma %*% l.g\n}\n\nhist(var.g.vec/(var.g.vec + var.cell.asthma), main=\"Relative Variance of Residuals\", ylab=\"Frequency\", xlab=\"sigma_g^2/(sigma_g^2 + alpha_asthma)\")\n```\n\nWe conlcude that in THIS data set, the error due to cell type is trivial in comparison to the residual error in our model.\n\n##Determine Proportion of Effect Due to Cell Type\nRecall that in our basice model for methylation,\n\\[\nY_{p \\times n} = \\left( B + L\\Omega^T \\right)X + \\tilde{E}_{p \\times n}.\n\\]\nSince we have estimates for $B, L$ and $\\Omega$, we can look at the proportion of of the effect due to $B$ and $L$ and use the metric $\\frac{\\Vert \\Omega\\ell_g \\Vert}{\\Vert \\Omega\\ell_g + \\beta_g \\Vert}$. If this is small, we can conclude that cell type is NOT confounding our coefficient estimates (for these data).\n\n```{r Proportion of Effect}\nprop.effect <- rep(0, m)\nfor (g in 1:m) {\n  prop.effect[g] <- sqrt( sum(LO[g,2:ncol(LO)] * LO[g,2:ncol(LO)])/sum( c(LO[g,2:ncol(LO)], B[g,])*c(LO[g,2:ncol(LO)], B[g,]) ) )\n}\nhist(prop.effect, breaks=90, main=\"Propotion of Effect Explained by Cell Type\", xlab=\"Proportion of Effect Explained by Cell Type\", ylab=\"Frequency\")\n```\n\n##Conclusions\n1.) We have a probability model to model cell types in individuals. This model has many practical benefits, since the we assume cell proportions are normally distributed with covariance structure related to the multinomial covariance structure. However, this model suffers from a few drawbacks:\n\n&ensp;&ensp;&ensp;i.) For the above estimated covariance structure, our prediction is not gauranteed to lie in the simplex (the smallest marginal probability of it lieing outside is ~0.09).\n\n&ensp;&ensp;&ensp;ii.) We incorrectly assume that the residuals are symmetric for points with mean close to the boundary of the simplex.\n\n2.) Whether or not our covariates determine cell type is very dependent on the data. In Michelle's Hutterite data, we show that covariates are NOT contaminated by cell type and that error due to cell type is negligible (when $L$ and $\\Omega$ are known).\n\nBased on this, I would argure a cell type training set is most appropriate to performa analysis. I think we lose too much information and make overly-conservative assumptions in an unsupervised approach.\n\n##Investigating Test Statistics\n\nIn this next part I will investigate possible test statistics, given that we have cell type information on some of the individuals individuals, but not all. Suppose there are $n_1$ individuals who's cell type is known and $n_2$ with unknown cell type. Recall that for $C_2 = X_2^T \\Omega + \\Xi^T$, the model is\n\\[\nY_1 = B_{p \\times d} X_1 + L_{p \\times K}C_1^T + E_1\n\\]\n\\[\nY_2 = BX_2 + L\\Omega^T X_2  + L\\Xi_{K \\times n_2} + E_2\n\\]\nwhere $X_i \\in \\mathbb{R}^{d \\times n_i}$, $C_1 \\in \\mathbb{R}^{n_1 \\times K}$, $\\Omega \\in \\mathbb{R}^{d \\times K}$ and $E_i \\sim MN_{p \\times n_i}\\left(0, \\Sigma_{p \\times p}, I_{n_i}\\right)$. Further, for $l$ the log-likelihood function of $p\\left( C_1 \\mid \\Omega \\right)$, define\n\\[\n-\\left(\\nabla^2 l\\left(\\text{vec}_{\\text{cols}}(\\Omega)\\right) \\mid_{\\hat{\\Omega}_{\\text{MLE}}}\\right)^{-1} = \\left[\\begin{matrix}\nS_{11} & \\cdots & S_{1K}\\\\\n\\vdots & \\ddots & \\vdots\\\\\nS_{K1} & \\cdots & S_{KK}\n\\end{matrix}\\right],\n\\]\n\\[\nS_i = \\left[\\begin{matrix}\nx_i^T S_{11} x_i & \\cdots & x_i^T S_{1K} x_i\\\\\n\\vdots & \\ddots & \\vdots\\\\\nx_i^T S_{K1} x_i & \\cdots & x_i^T S_{KK} x_i\n\\end{matrix}\\right]\n\\]\nand\n\\[\nS_{i \\times j} = \\left[\\begin{matrix}\nx_i^T S_{11} x_j & \\cdots & x_i^T S_{1K} x_j\\\\\n\\vdots & \\ddots & \\vdots\\\\\nx_i^T S_{K1} x_j & \\cdots & x_i^T S_{KK} x_j\n\\end{matrix}\\right]\n\\]\nand\n\\[\n\\Xi_{K \\times n_2} = \\left( \\xi_1 \\cdots \\xi_{n_2} \\right) \\quad \\xi_i \\sim N_K\\left( 0, R_i \\right)\n\\]\nwhere $R_i$ is the natural variance about the mean cell type and depends on $\\Omega^T x_i$, the mean cell type proportions for individual $i$. \n\nIn the estimation procedure, we want to CONDITION ON $C_1$, since these cell type data will presumably be from individuals with methylation data (that we obviously want to utilize). The estimation procedure would then be:\n\n1.) Use $C_1$ to estimate $\\Omega$ via maximum likelihood using the model multivariate normal model\n\\[\nC_1 = X_1^T \\Omega + \\Xi_1.\n\\]\nCall the maximum likelihood estimate $\\hat{\\Omega}_{\\text{MLE}}$ (code for this procedure has been implemented and tested). Note that since we are conditioning on $C_1$, we cannot use the Fisher information matrix as a measure of our uncertainty of $\\hat{\\Omega}_{\\text{MLE}}$. Instead, we would like to estimate the posterior distribution $p\\left(\\Omega \\mid C_1,X_1 \\right) \\propto p\\left( C_1 \\mid \\Omega,X_1\\right)p(\\Omega \\mid X_1)$. Since $\\Omega \\mid X_1$ is constrained to the interior of the simplex, our prior on $\\Omega \\mid X_1$ is uniform over the interior of the simplex and 0 outside of it. Therefore,\n\\[\np\\left(\\Omega \\mid C_1,X_1 \\right) \\propto p\\left( C_1 \\mid \\Omega,X_1\\right).\n\\]\nwhere we have that for $l\\left( \\Omega \\mid C_1, X_1\\right) = \\log p\\left( C_1 \\mid \\Omega,X_1\\right)$,\n\\[\nl\\left( \\Omega \\mid C_1, X_1\\right) = l\\left(\\hat{\\Omega}_{\\text{MLE}} \\mid C_1, X_1 \\right) + \\frac{1}{2}\\left( \\Omega - \\hat{\\Omega}_{\\text{MLE}}\\right)^T \\nabla^2_{\\Omega} l\\left(\\hat{\\Omega}_{\\text{MLE}} \\right)\\left( \\Omega - \\hat{\\Omega}_{\\text{MLE}}\\right) + o\\left( \\Vert \\Omega - \\hat{\\Omega}_{\\text{MLE}} \\Vert^2\\right).\n\\]\nTherefore, provided $l\\left( \\Omega \\mid C_1, X_1\\right)$ is concave, (or at least the likelihood of other modes goes down to 0 as $n_1 \\to \\infty$), then\n\\[\np\\left(\\Omega \\mid C_1,X_1 \\right) \\approx N\\left( \\hat{\\Omega}_{\\text{MLE}}, -\\left( \\nabla^2_{\\Omega} l\\left(\\hat{\\Omega}_{\\text{MLE}} \\right) \\right)^{-1} \\right).\n\\]\nNote that if we restrict ourselves to an $\\epsilon$ region AWAY from the boundary of the simplex, then I am confident we can show that for $n_1$ large enough the log-likelihood $l\\left( \\Omega \\mid C_1, X_1\\right)$ is concave in this region (with high probability). I then propose we use\n\\[\n\\Omega \\mid C_1,X_1 \\sim N\\left( \\hat{\\Omega}_{\\text{MLE}}, -\\left( \\nabla^2_{\\Omega} l\\left(\\hat{\\Omega}_{\\text{MLE}} \\right) \\right)^{-1} \\right)\n\\]\nas the posterior distribution of $\\Omega \\mid C_1, X_1$ in order to estimate our uncertainty in the estimate $\\hat{\\Omega}_{\\text{MLE}}$. This is very important when computing the variance of our estimator $\\hat{B}$ with small $n_1$.\n\n2.) Let $V_{X_1}$ be an orthonormal basis for the kernel of $X_1$, which has $n_1 - d$ dimensions. Let \n\\[\n\\tilde{Y}_1 = Y_1 V_{X_1} = L C_1^T V_{X_1} + \\underbrace{\\tilde{E}_1}_{MN_{p \\times n_1 - d}\\left(0, \\Sigma, I_{n_1 - d} \\right)}\n\\]\nand estimate $L$ on $n_1 - d - K$ degrees of freedom. Call the estimate $\\hat{L}$. Note that we can also get an estimate for $\\hat{\\Sigma}_{p \\times p}$, assuming it is diagonal. Note that we could also get an estimate of $\\hat{L}$ and $\\hat{\\Sigma}$ via OLS, including $X_1$ as a covariate.\nIn the first case\n\\[\n\\hat{L} \\mid C_1,X_1 \\sim MN_{p \\times K}\\left( L,\\Sigma_p,\\left( C_1^T V_{X_1} V_{X_1}^T C_1\\right)^{-1} \\right)\n\\]\nand in the second case (i.e. OLS to find $\\hat{L}, \\hat{B}$), $\\hat{L} = Y_1 P_{X_1^T}^{\\perp} C_1 D^{-1}$ and\n\\[\n\\hat{L} \\mid C_1,X_1 \\sim MN_{p \\times K}\\left( L,\\Sigma_p,\\left( C_1^T P_{X_1^T}^{\\perp} C_1\\right)^{-1} \\right)\n\\]\nwhere $D = C_1^T P^{\\perp}_{X_1^T}C_1$ and $P^{\\perp}_{X_1^T}$ is the projection matrix onto the orhtogonal compliment of the columns of $X_1^T$. Therefore, the two estimators are the SAME. Note that $\\hat{L}$ is an unbiased estimator of $L$.\n\n3.) Let $V_{C_1^T}$ be a basis for the kernel of $C_1^T$, which has dimension $n_2 - K$. Denote \n\\[\nZ_1 = Y_1 V_{C_1^T} \\mid C_1, X_1, X_2 \\sim MN_{p \\times n_1 - K}\\left(BX_1 V_{C_1^T}, \\Sigma, I_{n_1 - K} \\right)\n\\]\n\n4.) Let $Z_2 = Y_2 - \\hat{L}\\hat{\\Omega}_{\\text{MLE}} X_2$ be the residuals after removing the effect of the predicted mean cell type. If we condition on $C_1$, $\\Omega$ is random. In this procedure, we assume that $\\Omega \\mid C_1$ and $\\hat{L} \\mid C_1$ are INDEPENDENT. We then have that $Z_1$ and $Z_2$ are unbiased estimators for $BX_1 V_{C_1^T}$ and $BX_2$:\n\\[\nE\\left( Z_1 \\mid C_1, X_1, X_2 \\right) = BX_1 V_{C_1^T}\n\\]\n\\[\nE\\left(Z_2 \\mid C_1, X_1, X_2 \\right) = BX_2 + L\\hat{\\Omega}_{\\text{MLE}}^T X_2 - L\\hat{\\Omega}_{\\text{MLE}}^T X_2 = BX_2.\n\\]\nWe then must compute the VARIANCE of $\\left[ \\begin{matrix}\nZ_1 & Z_2\n\\end{matrix} \\right]$:\n\ni.) $\\text{Var}\\left( Z_1 \\mid C_1, X_1, X_2 \\right)$:\n\\[\n\\text{Var}\\left( Z_1 \\mid C_1, X_1, X_2 \\right) = \\Sigma_p \\otimes I_{n_1 - K}\n\\]\nii.) $\\text{Var}\\left( Z_2 \\mid C_1, X_1, X_2 \\right)$:\n\\[\n\\text{Var}\\left( \\underbrace{\\left( Z_2 \\right)_i}_{\\text{$i^{\\text{th}}$ column of $Z_2$}} \\mid C_1, X_1, X_2 \\right) = \\text{Var}\\left( \\left( Y_2 \\right)_i - \\hat{L} \\hat{\\Omega}_{\\text{MLE}}^T x_i \\mid C_1, X_1, X_2\\right) = \\text{Var}\\left( \\left( L\\Omega^T-\\hat{L}\\hat{\\Omega}_{\\text{MLE}}^T \\right) x_i  + L \\xi_i + \\left( E_2 \\right)_i \\mid C_1, X_1, X_2 \\right)\n\\]\n\\[\n= \\text{Var}\\left( \\left( L\\Omega^T-\\hat{L}\\hat{\\Omega}_{\\text{MLE}}^T \\right) x_i  + L \\xi_i \\mid C_1, X_1, X_2 \\right) + \\Sigma_{p \\times p} = E_{\\Omega}\\text{Var}\\left( \\left( L\\Omega^T-\\hat{L}\\hat{\\Omega}_{\\text{MLE}}^T \\right) x_i  + L \\xi_i \\mid C_1, X_1, X_2, \\Omega \\right) + \\cdots\n\\]\n\\[\n\\cdots \\text{Var}_{\\Omega} E \\left[ \\left( L\\Omega^T-\\hat{L}\\hat{\\Omega}_{\\text{MLE}}^T \\right) x_i  + L \\xi_i \\mid C_1, X_1, X_2, \\Omega \\right] + \\Sigma_{p \\times p} = \\Sigma_{p \\times p}  x_i^T \\hat{\\Omega}_{\\text{MLE}} \\left( C_1^T P^{\\perp}_{X_1^T} C_1 \\right)^{-1}\\hat{\\Omega}_{\\text{MLE}}^T x_i + \\cdots\n\\]\n\\[\n\\cdots L E\\left[ R_i\\left( \\Omega^T x_i \\right) \\mid C_1, X_1, X_2 \\right]L^T + L S_i L^T = \\Sigma_p \\left[ 1 + x_i^T \\hat{\\Omega}_{\\text{MLE}} \\left( C_1^T P^{\\perp}_{X_1^T} C_1 \\right)^{-1}\\hat{\\Omega}_{\\text{MLE}}^T x_i \\right] + L\\left( E\\left[ R_i\\left( \\Omega^T x_i \\right) \\mid C_1, X_1, X_2 \\right] + S_i \\right)L^T\n\\]\nAnd,\n\\[\n\\text{Cov}\\left( \\left( Z_2 \\right)_i, \\left( Z_2 \\right)_j \\mid C_1, X_1, X_2 \\right) = \\text{Cov}\\left( \\left( L - \\hat{L} \\right)\\Omega^T x_i + L \\xi_i + \\left( E_2 \\right)_i, \\left( L - \\hat{L} \\right)\\Omega^T x_j + L \\xi_j + \\left( E_2 \\right)_j \\mid C_1, X_1, X_2 \\right)\n\\]\n\\[\n \\underbrace{=}_{\\text{Same procedure as above}} L S_{i \\times j} L^T + \\Sigma_p\\left[ x_i^T \\hat{\\Omega}_{\\text{MLE}} \\left( C_1^T P^{\\perp}_{X_1^T} C_1 \\right)^{-1}\\hat{\\Omega}_{\\text{MLE}}^T x_j \\right]\n\\]\n\niii.) $\\text{Cov}\\left( Z_1, Z_2 \\mid C_1, X_1, X_2 \\right)$:\n\\[\n\\text{Cov}\\left( Z_1, Z_2 \\mid C_1, X_1, X_2 \\right) \\underbrace{=}_{\\text{Steps Omitted}} - \\Sigma_{p \\times p} \\otimes \\left[ V_{C_1^T}^T P^{\\perp}_{X_1^T} C_1 \\left( C_1^T P^{\\perp}_{X_1^T} C_1 \\right)^{-1} \\hat{\\Omega}_{\\text{MLE}} X_2 \\right]\n\\]\n\nThese calculations make sense, since in the limit as $n_1, n_2 \\to \\infty$, the covariance between columns disappears and we are left with a variance term that we would use if we knew $L$ and $\\Omega$.\n\n5.) Using the above calculations, we can compute the covariance matrix $U_g \\in \\mathbb{R}^{\\left( n_1 -K + n_2 \\right) \\times \\left( n_1 -K + n_2 \\right)}$ for a single site $g$ across individuals. If $\\beta_g$ is the $g^{\\text{th}}$ row of $B \\in \\mathbb{R}^{p \\times d}$, $\\ell_g$ the $g^{\\text{th}}$ row of $L \\mathbb{R}^{p \\times K}$ and $Y_i^{(g)}$ the $i^{\\text{th}}$ row of $Y_i \\in \\mathbb{R}^{p \\times n_i}$, then\n\\[\n\\hat{\\beta}_g = \\left( \\left[ \\begin{matrix}\nX_1 V_{C_1^T} & X_2\n\\end{matrix} \\right] U_g^{-1} \\left[ \\begin{matrix}\nV_{C_1^T}^T X_1^T\\\\\nX_2^T\n\\end{matrix} \\right]\\right)^{-1} \\left[ \\begin{matrix}\nX_1 V_{C_1^T} & X_2\n\\end{matrix} \\right] U_g^{-1} \\left[ \\begin{matrix}\nY_1^{(g)} V_{C_1^T}^T\\\\\nY_2^{(g)} - X_2^T \\hat{\\Omega}_{\\text{MLE}} \\hat{\\ell}_g\n\\end{matrix} \\right]\n\\]\nwhere\n\\[\nE \\hat{\\beta}_g = \\beta_g\n\\]\nand\n\\[\n\\text{Var}\\left( \\hat{\\beta}_g \\mid C_1, X_1, X_2 \\right) = \\left( \\left[ \\begin{matrix}\nX_1 V_{C_1^T} & X_2\n\\end{matrix} \\right] U_g^{-1} \\left[ \\begin{matrix}\nV_{C_1^T}^T X_1^T\\\\\nX_2^T\n\\end{matrix} \\right]\\right)^{-1} \\stackrel{n_1,n_2 \\to \\infty}{\\longrightarrow} \\left( \\sigma_g^{-2} X_1 P^{\\perp}_{C_1^T} X_1^T + \\left( \\sigma_g^2 + \\ell_g^T R_i \\ell_g \\right)^{-1} X_2 X_2^T \\right)^{-1}.\n\\]\nNote that if we find that the cell proportions are uncorrelated with methylation, then $\\ell_g$ would be negligible in comparison to $\\sigma_g^2$, which would be the standard regression problem where $B$ was the only coefficient. I think I can also prove that the under regularity conditions, the asymptotic distrubution for $\\hat{\\beta}_g$ is normally distributed.\n\nNote that we can also simultaneously compute GLS estimates for the multiple sites coefficient vector $\\left[ \\begin{matrix}\n\\beta_{g_1}\\\\\n\\vdots\\\\\n\\beta_{g_m}\n\\end{matrix} \\right]$ if we assume that $\\Sigma_{p \\times p}$ is diagonal. The analytic form is a bit messy, however.\n\n## Session information\n\n```{r info}\nsessionInfo()\n```\n",
    "created" : 1455034057170.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2490819356",
    "id" : "4A930E74",
    "lastKnownWriteTime" : 1457370484,
    "path" : "~/Desktop/Uchicago/Nicolae/GitWork/CellType/analysis/InvestigateCellType.Rmd",
    "project_path" : "InvestigateCellType.Rmd",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}